{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victordaniel/DEEP-LEARNIG-COURSE/blob/main/Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Define the transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load the dataset\n",
        "train_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Create DataLoader\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "# Define a simple CNN model\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, 1)\n",
        "        self.fc1 = nn.Linear(32 * 6 * 6, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.max_pool2d(x, 2)\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = torch.max_pool2d(x, 2)\n",
        "        x = x.view(-1, 32 * 6 * 6)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model, loss function, and optimizer\n",
        "model = SimpleCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Move tensors to the configured device\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass, backward pass, and optimize\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n"
      ],
      "metadata": {
        "id": "FznEGwCj_z6W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d9f7775-30d4-4db7-e6dc-a0424d950587"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:05<00:00, 33845356.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Epoch [1/5], Step [100/782], Loss: 1.7779\n",
            "Epoch [1/5], Step [200/782], Loss: 1.6873\n",
            "Epoch [1/5], Step [300/782], Loss: 1.5136\n",
            "Epoch [1/5], Step [400/782], Loss: 1.4466\n",
            "Epoch [1/5], Step [500/782], Loss: 1.3792\n",
            "Epoch [1/5], Step [600/782], Loss: 1.3298\n",
            "Epoch [1/5], Step [700/782], Loss: 1.3452\n",
            "Epoch [2/5], Step [100/782], Loss: 1.0866\n",
            "Epoch [2/5], Step [200/782], Loss: 1.3736\n",
            "Epoch [2/5], Step [300/782], Loss: 0.9734\n",
            "Epoch [2/5], Step [400/782], Loss: 1.0109\n",
            "Epoch [2/5], Step [500/782], Loss: 0.9515\n",
            "Epoch [2/5], Step [600/782], Loss: 1.2780\n",
            "Epoch [2/5], Step [700/782], Loss: 1.1926\n",
            "Epoch [3/5], Step [100/782], Loss: 1.1281\n",
            "Epoch [3/5], Step [200/782], Loss: 1.0282\n",
            "Epoch [3/5], Step [300/782], Loss: 1.0821\n",
            "Epoch [3/5], Step [400/782], Loss: 1.2290\n",
            "Epoch [3/5], Step [500/782], Loss: 0.9623\n",
            "Epoch [3/5], Step [600/782], Loss: 1.1268\n",
            "Epoch [3/5], Step [700/782], Loss: 1.1562\n",
            "Epoch [4/5], Step [100/782], Loss: 1.1430\n",
            "Epoch [4/5], Step [200/782], Loss: 0.8478\n",
            "Epoch [4/5], Step [300/782], Loss: 0.9734\n",
            "Epoch [4/5], Step [400/782], Loss: 0.9176\n",
            "Epoch [4/5], Step [500/782], Loss: 1.0728\n",
            "Epoch [4/5], Step [600/782], Loss: 1.0510\n",
            "Epoch [4/5], Step [700/782], Loss: 0.8011\n",
            "Epoch [5/5], Step [100/782], Loss: 0.7323\n",
            "Epoch [5/5], Step [200/782], Loss: 0.8304\n",
            "Epoch [5/5], Step [300/782], Loss: 0.7769\n",
            "Epoch [5/5], Step [400/782], Loss: 0.8133\n",
            "Epoch [5/5], Step [500/782], Loss: 0.8140\n",
            "Epoch [5/5], Step [600/782], Loss: 0.9304\n",
            "Epoch [5/5], Step [700/782], Loss: 0.8612\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5GK_b1dsADka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vQkXbIwuDNEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Extension #load model #save model #eval #Print accuracy"
      ],
      "metadata": {
        "id": "t3QUel0GDNY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Define the transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load the dataset\n",
        "train_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Create DataLoader\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "# Define a simple CNN model\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, 1)\n",
        "        self.fc1 = nn.Linear(32 * 6 * 6, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.max_pool2d(x, 2)\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = torch.max_pool2d(x, 2)\n",
        "        x = x.view(-1, 32 * 6 * 6)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model, loss function, and optimizer\n",
        "model = SimpleCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Function to save the model\n",
        "def save_model(epoch, model, optimizer, loss, path):\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss,\n",
        "    }, path)\n",
        "\n",
        "# Function to load the model\n",
        "def load_model(path, model, optimizer):\n",
        "    checkpoint = torch.load(path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    epoch = checkpoint['epoch']\n",
        "    loss = checkpoint['loss']\n",
        "    return epoch, loss\n",
        "\n",
        "# Training and validation\n",
        "num_epochs = 5\n",
        "best_loss = float('inf')\n",
        "model_save_path = './model.pth'\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if (i+1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {epoch_loss:.4f}')\n",
        "\n",
        "    # Save the model if the validation loss is the best we've seen so far.\n",
        "    if epoch_loss < best_loss:\n",
        "        best_loss = epoch_loss\n",
        "        save_model(epoch, model, optimizer, epoch_loss, model_save_path)\n",
        "\n",
        "# Validation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'Accuracy of the model on the test images: {100 * correct / total} %')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdSnQ4qNDVKJ",
        "outputId": "1b81fac6-ca3f-4068-b09f-7bcf2644dc00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:18<00:00, 9226921.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Epoch [1/5], Step [100/782], Loss: 1.5238\n",
            "Epoch [1/5], Step [200/782], Loss: 1.5151\n",
            "Epoch [1/5], Step [300/782], Loss: 1.6491\n",
            "Epoch [1/5], Step [400/782], Loss: 1.2008\n",
            "Epoch [1/5], Step [500/782], Loss: 1.5274\n",
            "Epoch [1/5], Step [600/782], Loss: 1.3187\n",
            "Epoch [1/5], Step [700/782], Loss: 1.2800\n",
            "Epoch [1/5], Average Loss: 1.4741\n",
            "Epoch [2/5], Step [100/782], Loss: 1.2894\n",
            "Epoch [2/5], Step [200/782], Loss: 1.2377\n",
            "Epoch [2/5], Step [300/782], Loss: 1.3691\n",
            "Epoch [2/5], Step [400/782], Loss: 1.2598\n",
            "Epoch [2/5], Step [500/782], Loss: 1.1166\n",
            "Epoch [2/5], Step [600/782], Loss: 1.0534\n",
            "Epoch [2/5], Step [700/782], Loss: 1.2542\n",
            "Epoch [2/5], Average Loss: 1.1440\n",
            "Epoch [3/5], Step [100/782], Loss: 1.1997\n",
            "Epoch [3/5], Step [200/782], Loss: 1.0584\n",
            "Epoch [3/5], Step [300/782], Loss: 0.8691\n",
            "Epoch [3/5], Step [400/782], Loss: 0.9212\n",
            "Epoch [3/5], Step [500/782], Loss: 0.8295\n",
            "Epoch [3/5], Step [600/782], Loss: 1.1126\n",
            "Epoch [3/5], Step [700/782], Loss: 1.0631\n",
            "Epoch [3/5], Average Loss: 0.9983\n",
            "Epoch [4/5], Step [100/782], Loss: 0.9900\n",
            "Epoch [4/5], Step [200/782], Loss: 0.7949\n",
            "Epoch [4/5], Step [300/782], Loss: 0.8359\n",
            "Epoch [4/5], Step [400/782], Loss: 1.0655\n",
            "Epoch [4/5], Step [500/782], Loss: 1.0827\n",
            "Epoch [4/5], Step [600/782], Loss: 0.8893\n",
            "Epoch [4/5], Step [700/782], Loss: 0.8919\n",
            "Epoch [4/5], Average Loss: 0.9024\n",
            "Epoch [5/5], Step [100/782], Loss: 0.7642\n",
            "Epoch [5/5], Step [200/782], Loss: 0.7100\n",
            "Epoch [5/5], Step [300/782], Loss: 0.8209\n",
            "Epoch [5/5], Step [400/782], Loss: 0.8134\n",
            "Epoch [5/5], Step [500/782], Loss: 0.9223\n",
            "Epoch [5/5], Step [600/782], Loss: 0.8841\n",
            "Epoch [5/5], Step [700/782], Loss: 0.8316\n",
            "Epoch [5/5], Average Loss: 0.8267\n",
            "Accuracy of the model on the test images: 68.27 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ACCURACY to be improved , dropuout ,enhanced architecture ,data augmentation, learnig rate scheduler is added."
      ],
      "metadata": {
        "id": "tvtqHwzsEkvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Define the transform with data augmentation\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load the dataset\n",
        "train_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Create DataLoader\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "# Define an enhanced CNN model\n",
        "class EnhancedCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EnhancedCNN, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=3),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2))\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2))\n",
        "        self.fc1 = nn.Linear(128*2*2, 128)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc1(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "# Instantiate the model, loss function, optimizer, and scheduler\n",
        "model = EnhancedCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
        "\n",
        "# Function to save the model\n",
        "def save_model(epoch, model, optimizer, loss, path):\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss,\n",
        "    }, path)\n",
        "\n",
        "# Function to load the model\n",
        "def load_model(path, model, optimizer):\n",
        "    checkpoint = torch.load(path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    epoch = checkpoint['epoch']\n",
        "    loss = checkpoint['loss']\n",
        "    return epoch, loss\n",
        "\n",
        "# Training and validation\n",
        "num_epochs = 50\n",
        "best_loss = float('inf')\n",
        "model_save_path = './model.pth'\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if (i+1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {epoch_loss:.4f}')\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    # Save the model if the validation loss is the best we've seen so far.\n",
        "    if epoch_loss < best_loss:\n",
        "        best_loss = epoch_loss\n",
        "        save_model(epoch, model, optimizer, epoch_loss, model_save_path)\n",
        "\n",
        "# Validation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'Accuracy of the model on the test images: {100 * correct / total} %')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvUX_Va_E7yZ",
        "outputId": "7c0e871c-d575-49f1-ba01-0aad78844c86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch [1/50], Step [100/782], Loss: 1.8477\n",
            "Epoch [1/50], Step [200/782], Loss: 1.6041\n",
            "Epoch [1/50], Step [300/782], Loss: 1.4495\n",
            "Epoch [1/50], Step [400/782], Loss: 1.3800\n",
            "Epoch [1/50], Step [500/782], Loss: 1.5237\n",
            "Epoch [1/50], Step [600/782], Loss: 1.3321\n",
            "Epoch [1/50], Step [700/782], Loss: 1.5248\n",
            "Epoch [1/50], Average Loss: 1.4589\n",
            "Epoch [2/50], Step [100/782], Loss: 1.1387\n",
            "Epoch [2/50], Step [200/782], Loss: 1.0612\n",
            "Epoch [2/50], Step [300/782], Loss: 1.0934\n",
            "Epoch [2/50], Step [400/782], Loss: 1.0757\n",
            "Epoch [2/50], Step [500/782], Loss: 0.9908\n",
            "Epoch [2/50], Step [600/782], Loss: 1.0988\n",
            "Epoch [2/50], Step [700/782], Loss: 1.0933\n",
            "Epoch [2/50], Average Loss: 1.1519\n",
            "Epoch [3/50], Step [100/782], Loss: 1.1490\n",
            "Epoch [3/50], Step [200/782], Loss: 0.6958\n",
            "Epoch [3/50], Step [300/782], Loss: 0.9447\n",
            "Epoch [3/50], Step [400/782], Loss: 1.1636\n",
            "Epoch [3/50], Step [500/782], Loss: 0.8200\n",
            "Epoch [3/50], Step [600/782], Loss: 0.9181\n",
            "Epoch [3/50], Step [700/782], Loss: 0.8702\n",
            "Epoch [3/50], Average Loss: 1.0295\n",
            "Epoch [4/50], Step [100/782], Loss: 0.9126\n",
            "Epoch [4/50], Step [200/782], Loss: 0.8423\n",
            "Epoch [4/50], Step [300/782], Loss: 0.8643\n",
            "Epoch [4/50], Step [400/782], Loss: 1.0093\n",
            "Epoch [4/50], Step [500/782], Loss: 0.9884\n",
            "Epoch [4/50], Step [600/782], Loss: 0.8812\n",
            "Epoch [4/50], Step [700/782], Loss: 0.7812\n",
            "Epoch [4/50], Average Loss: 0.9564\n",
            "Epoch [5/50], Step [100/782], Loss: 0.8619\n",
            "Epoch [5/50], Step [200/782], Loss: 0.9272\n",
            "Epoch [5/50], Step [300/782], Loss: 0.9802\n",
            "Epoch [5/50], Step [400/782], Loss: 1.0721\n",
            "Epoch [5/50], Step [500/782], Loss: 0.8323\n",
            "Epoch [5/50], Step [600/782], Loss: 0.7521\n",
            "Epoch [5/50], Step [700/782], Loss: 0.9213\n",
            "Epoch [5/50], Average Loss: 0.8984\n",
            "Epoch [6/50], Step [100/782], Loss: 0.6534\n",
            "Epoch [6/50], Step [200/782], Loss: 1.0865\n",
            "Epoch [6/50], Step [300/782], Loss: 0.7126\n",
            "Epoch [6/50], Step [400/782], Loss: 1.0197\n",
            "Epoch [6/50], Step [500/782], Loss: 0.7987\n",
            "Epoch [6/50], Step [600/782], Loss: 0.8283\n",
            "Epoch [6/50], Step [700/782], Loss: 0.8519\n",
            "Epoch [6/50], Average Loss: 0.8537\n",
            "Epoch [7/50], Step [100/782], Loss: 0.8646\n",
            "Epoch [7/50], Step [200/782], Loss: 0.8472\n",
            "Epoch [7/50], Step [300/782], Loss: 0.6965\n",
            "Epoch [7/50], Step [400/782], Loss: 0.7313\n",
            "Epoch [7/50], Step [500/782], Loss: 0.8082\n",
            "Epoch [7/50], Step [600/782], Loss: 0.9856\n",
            "Epoch [7/50], Step [700/782], Loss: 0.6003\n",
            "Epoch [7/50], Average Loss: 0.8197\n",
            "Epoch [8/50], Step [100/782], Loss: 0.8345\n",
            "Epoch [8/50], Step [200/782], Loss: 0.7295\n",
            "Epoch [8/50], Step [300/782], Loss: 0.8394\n",
            "Epoch [8/50], Step [400/782], Loss: 0.8406\n",
            "Epoch [8/50], Step [500/782], Loss: 0.9387\n",
            "Epoch [8/50], Step [600/782], Loss: 0.6406\n",
            "Epoch [8/50], Step [700/782], Loss: 0.7058\n",
            "Epoch [8/50], Average Loss: 0.7893\n",
            "Epoch [9/50], Step [100/782], Loss: 0.6998\n",
            "Epoch [9/50], Step [200/782], Loss: 0.8590\n",
            "Epoch [9/50], Step [300/782], Loss: 1.0385\n",
            "Epoch [9/50], Step [400/782], Loss: 0.9863\n",
            "Epoch [9/50], Step [500/782], Loss: 0.8540\n",
            "Epoch [9/50], Step [600/782], Loss: 0.7265\n",
            "Epoch [9/50], Step [700/782], Loss: 0.7936\n",
            "Epoch [9/50], Average Loss: 0.7678\n",
            "Epoch [10/50], Step [100/782], Loss: 0.8222\n",
            "Epoch [10/50], Step [200/782], Loss: 0.6527\n",
            "Epoch [10/50], Step [300/782], Loss: 0.6725\n",
            "Epoch [10/50], Step [400/782], Loss: 1.0081\n",
            "Epoch [10/50], Step [500/782], Loss: 0.6876\n",
            "Epoch [10/50], Step [600/782], Loss: 0.7154\n",
            "Epoch [10/50], Step [700/782], Loss: 0.8293\n",
            "Epoch [10/50], Average Loss: 0.7506\n",
            "Epoch [11/50], Step [100/782], Loss: 0.7384\n",
            "Epoch [11/50], Step [200/782], Loss: 0.9092\n",
            "Epoch [11/50], Step [300/782], Loss: 0.7192\n",
            "Epoch [11/50], Step [400/782], Loss: 0.6686\n",
            "Epoch [11/50], Step [500/782], Loss: 0.5583\n",
            "Epoch [11/50], Step [600/782], Loss: 0.6671\n",
            "Epoch [11/50], Step [700/782], Loss: 0.7207\n",
            "Epoch [11/50], Average Loss: 0.7266\n",
            "Epoch [12/50], Step [100/782], Loss: 0.8435\n",
            "Epoch [12/50], Step [200/782], Loss: 0.7672\n",
            "Epoch [12/50], Step [300/782], Loss: 1.0531\n",
            "Epoch [12/50], Step [400/782], Loss: 0.6238\n",
            "Epoch [12/50], Step [500/782], Loss: 0.7925\n",
            "Epoch [12/50], Step [600/782], Loss: 0.8654\n",
            "Epoch [12/50], Step [700/782], Loss: 0.5102\n",
            "Epoch [12/50], Average Loss: 0.7090\n",
            "Epoch [13/50], Step [100/782], Loss: 0.5770\n",
            "Epoch [13/50], Step [200/782], Loss: 0.5123\n",
            "Epoch [13/50], Step [300/782], Loss: 0.8405\n",
            "Epoch [13/50], Step [400/782], Loss: 0.6585\n",
            "Epoch [13/50], Step [500/782], Loss: 0.9852\n",
            "Epoch [13/50], Step [600/782], Loss: 0.6763\n",
            "Epoch [13/50], Step [700/782], Loss: 0.7314\n",
            "Epoch [13/50], Average Loss: 0.6924\n",
            "Epoch [14/50], Step [100/782], Loss: 0.8054\n",
            "Epoch [14/50], Step [200/782], Loss: 0.6974\n",
            "Epoch [14/50], Step [300/782], Loss: 0.4512\n",
            "Epoch [14/50], Step [400/782], Loss: 0.6771\n",
            "Epoch [14/50], Step [500/782], Loss: 0.8216\n",
            "Epoch [14/50], Step [600/782], Loss: 0.7710\n",
            "Epoch [14/50], Step [700/782], Loss: 0.5129\n",
            "Epoch [14/50], Average Loss: 0.6852\n",
            "Epoch [15/50], Step [100/782], Loss: 0.3942\n",
            "Epoch [15/50], Step [200/782], Loss: 0.6232\n",
            "Epoch [15/50], Step [300/782], Loss: 0.5638\n",
            "Epoch [15/50], Step [400/782], Loss: 0.8147\n",
            "Epoch [15/50], Step [500/782], Loss: 0.5853\n",
            "Epoch [15/50], Step [600/782], Loss: 0.6108\n",
            "Epoch [15/50], Step [700/782], Loss: 0.5514\n",
            "Epoch [15/50], Average Loss: 0.6706\n",
            "Epoch [16/50], Step [100/782], Loss: 0.8990\n",
            "Epoch [16/50], Step [200/782], Loss: 0.7178\n",
            "Epoch [16/50], Step [300/782], Loss: 0.7668\n",
            "Epoch [16/50], Step [400/782], Loss: 0.5481\n",
            "Epoch [16/50], Step [500/782], Loss: 0.7993\n",
            "Epoch [16/50], Step [600/782], Loss: 0.6084\n",
            "Epoch [16/50], Step [700/782], Loss: 0.6679\n",
            "Epoch [16/50], Average Loss: 0.6555\n",
            "Epoch [17/50], Step [100/782], Loss: 0.4672\n",
            "Epoch [17/50], Step [200/782], Loss: 0.7981\n",
            "Epoch [17/50], Step [300/782], Loss: 0.8736\n",
            "Epoch [17/50], Step [400/782], Loss: 0.6149\n",
            "Epoch [17/50], Step [500/782], Loss: 0.8061\n",
            "Epoch [17/50], Step [600/782], Loss: 0.4900\n",
            "Epoch [17/50], Step [700/782], Loss: 0.5741\n",
            "Epoch [17/50], Average Loss: 0.6486\n",
            "Epoch [18/50], Step [100/782], Loss: 0.6681\n",
            "Epoch [18/50], Step [200/782], Loss: 0.4504\n",
            "Epoch [18/50], Step [300/782], Loss: 0.4965\n",
            "Epoch [18/50], Step [400/782], Loss: 0.5790\n",
            "Epoch [18/50], Step [500/782], Loss: 0.5002\n",
            "Epoch [18/50], Step [600/782], Loss: 0.7414\n",
            "Epoch [18/50], Step [700/782], Loss: 0.7756\n",
            "Epoch [18/50], Average Loss: 0.6424\n",
            "Epoch [19/50], Step [100/782], Loss: 0.5767\n",
            "Epoch [19/50], Step [200/782], Loss: 0.5347\n",
            "Epoch [19/50], Step [300/782], Loss: 0.5591\n",
            "Epoch [19/50], Step [400/782], Loss: 0.7521\n",
            "Epoch [19/50], Step [500/782], Loss: 0.5378\n",
            "Epoch [19/50], Step [600/782], Loss: 0.7242\n",
            "Epoch [19/50], Step [700/782], Loss: 0.6092\n",
            "Epoch [19/50], Average Loss: 0.6370\n",
            "Epoch [20/50], Step [100/782], Loss: 0.4238\n",
            "Epoch [20/50], Step [200/782], Loss: 0.6033\n",
            "Epoch [20/50], Step [300/782], Loss: 0.4656\n",
            "Epoch [20/50], Step [400/782], Loss: 0.4129\n",
            "Epoch [20/50], Step [500/782], Loss: 0.5183\n",
            "Epoch [20/50], Step [600/782], Loss: 0.5533\n",
            "Epoch [20/50], Step [700/782], Loss: 0.5808\n",
            "Epoch [20/50], Average Loss: 0.6248\n",
            "Epoch [21/50], Step [100/782], Loss: 0.5668\n",
            "Epoch [21/50], Step [200/782], Loss: 0.6165\n",
            "Epoch [21/50], Step [300/782], Loss: 0.4115\n",
            "Epoch [21/50], Step [400/782], Loss: 0.3398\n",
            "Epoch [21/50], Step [500/782], Loss: 0.4423\n",
            "Epoch [21/50], Step [600/782], Loss: 0.7127\n",
            "Epoch [21/50], Step [700/782], Loss: 0.4197\n",
            "Epoch [21/50], Average Loss: 0.5605\n",
            "Epoch [22/50], Step [100/782], Loss: 0.3887\n",
            "Epoch [22/50], Step [200/782], Loss: 0.4714\n",
            "Epoch [22/50], Step [300/782], Loss: 0.5579\n",
            "Epoch [22/50], Step [400/782], Loss: 0.6671\n",
            "Epoch [22/50], Step [500/782], Loss: 0.5662\n",
            "Epoch [22/50], Step [600/782], Loss: 0.6339\n",
            "Epoch [22/50], Step [700/782], Loss: 0.3905\n",
            "Epoch [22/50], Average Loss: 0.5414\n",
            "Epoch [23/50], Step [100/782], Loss: 0.5408\n",
            "Epoch [23/50], Step [200/782], Loss: 0.5267\n",
            "Epoch [23/50], Step [300/782], Loss: 0.5229\n",
            "Epoch [23/50], Step [400/782], Loss: 0.5125\n",
            "Epoch [23/50], Step [500/782], Loss: 0.4556\n",
            "Epoch [23/50], Step [600/782], Loss: 0.4833\n",
            "Epoch [23/50], Step [700/782], Loss: 0.7219\n",
            "Epoch [23/50], Average Loss: 0.5346\n",
            "Epoch [24/50], Step [100/782], Loss: 0.3784\n",
            "Epoch [24/50], Step [200/782], Loss: 0.6078\n",
            "Epoch [24/50], Step [300/782], Loss: 0.4273\n",
            "Epoch [24/50], Step [400/782], Loss: 0.4857\n",
            "Epoch [24/50], Step [500/782], Loss: 0.3093\n",
            "Epoch [24/50], Step [600/782], Loss: 0.4177\n",
            "Epoch [24/50], Step [700/782], Loss: 0.6812\n",
            "Epoch [24/50], Average Loss: 0.5279\n",
            "Epoch [25/50], Step [100/782], Loss: 0.5264\n",
            "Epoch [25/50], Step [200/782], Loss: 0.6043\n",
            "Epoch [25/50], Step [300/782], Loss: 0.7798\n",
            "Epoch [25/50], Step [400/782], Loss: 0.8173\n",
            "Epoch [25/50], Step [500/782], Loss: 0.8679\n",
            "Epoch [25/50], Step [600/782], Loss: 0.5170\n",
            "Epoch [25/50], Step [700/782], Loss: 0.4957\n",
            "Epoch [25/50], Average Loss: 0.5230\n",
            "Epoch [26/50], Step [100/782], Loss: 0.4705\n",
            "Epoch [26/50], Step [200/782], Loss: 0.6406\n",
            "Epoch [26/50], Step [300/782], Loss: 0.5701\n",
            "Epoch [26/50], Step [400/782], Loss: 0.6075\n",
            "Epoch [26/50], Step [500/782], Loss: 0.4857\n",
            "Epoch [26/50], Step [600/782], Loss: 0.6334\n",
            "Epoch [26/50], Step [700/782], Loss: 0.5821\n",
            "Epoch [26/50], Average Loss: 0.5234\n",
            "Epoch [27/50], Step [100/782], Loss: 0.5201\n",
            "Epoch [27/50], Step [200/782], Loss: 0.4488\n",
            "Epoch [27/50], Step [300/782], Loss: 0.4614\n",
            "Epoch [27/50], Step [400/782], Loss: 0.8217\n",
            "Epoch [27/50], Step [500/782], Loss: 0.8046\n",
            "Epoch [27/50], Step [600/782], Loss: 0.4995\n",
            "Epoch [27/50], Step [700/782], Loss: 0.5012\n",
            "Epoch [27/50], Average Loss: 0.5172\n",
            "Epoch [28/50], Step [100/782], Loss: 0.5651\n",
            "Epoch [28/50], Step [200/782], Loss: 0.5664\n",
            "Epoch [28/50], Step [300/782], Loss: 0.4918\n",
            "Epoch [28/50], Step [400/782], Loss: 0.4083\n",
            "Epoch [28/50], Step [500/782], Loss: 0.4191\n",
            "Epoch [28/50], Step [600/782], Loss: 0.6271\n",
            "Epoch [28/50], Step [700/782], Loss: 0.4489\n",
            "Epoch [28/50], Average Loss: 0.5119\n",
            "Epoch [29/50], Step [100/782], Loss: 0.4661\n",
            "Epoch [29/50], Step [200/782], Loss: 0.7586\n",
            "Epoch [29/50], Step [300/782], Loss: 0.4543\n",
            "Epoch [29/50], Step [400/782], Loss: 0.5544\n",
            "Epoch [29/50], Step [500/782], Loss: 0.4979\n",
            "Epoch [29/50], Step [600/782], Loss: 0.7392\n",
            "Epoch [29/50], Step [700/782], Loss: 0.5430\n",
            "Epoch [29/50], Average Loss: 0.5221\n",
            "Epoch [30/50], Step [100/782], Loss: 0.5369\n",
            "Epoch [30/50], Step [200/782], Loss: 0.5226\n",
            "Epoch [30/50], Step [300/782], Loss: 0.5764\n",
            "Epoch [30/50], Step [400/782], Loss: 0.5984\n",
            "Epoch [30/50], Step [500/782], Loss: 0.3920\n",
            "Epoch [30/50], Step [600/782], Loss: 0.3250\n",
            "Epoch [30/50], Step [700/782], Loss: 0.5463\n",
            "Epoch [30/50], Average Loss: 0.5054\n",
            "Epoch [31/50], Step [100/782], Loss: 0.5404\n",
            "Epoch [31/50], Step [200/782], Loss: 0.6274\n",
            "Epoch [31/50], Step [300/782], Loss: 0.4173\n",
            "Epoch [31/50], Step [400/782], Loss: 0.3982\n",
            "Epoch [31/50], Step [500/782], Loss: 0.7861\n",
            "Epoch [31/50], Step [600/782], Loss: 0.4689\n",
            "Epoch [31/50], Step [700/782], Loss: 0.4809\n",
            "Epoch [31/50], Average Loss: 0.5117\n",
            "Epoch [32/50], Step [100/782], Loss: 0.5001\n",
            "Epoch [32/50], Step [200/782], Loss: 0.8317\n",
            "Epoch [32/50], Step [300/782], Loss: 0.4183\n",
            "Epoch [32/50], Step [400/782], Loss: 0.5354\n",
            "Epoch [32/50], Step [500/782], Loss: 0.5878\n",
            "Epoch [32/50], Step [600/782], Loss: 0.5375\n",
            "Epoch [32/50], Step [700/782], Loss: 0.2999\n",
            "Epoch [32/50], Average Loss: 0.5014\n",
            "Epoch [33/50], Step [100/782], Loss: 0.2875\n",
            "Epoch [33/50], Step [200/782], Loss: 0.5400\n",
            "Epoch [33/50], Step [300/782], Loss: 0.6439\n",
            "Epoch [33/50], Step [400/782], Loss: 0.3604\n",
            "Epoch [33/50], Step [500/782], Loss: 0.6542\n",
            "Epoch [33/50], Step [600/782], Loss: 0.4560\n",
            "Epoch [33/50], Step [700/782], Loss: 0.4135\n",
            "Epoch [33/50], Average Loss: 0.5078\n",
            "Epoch [34/50], Step [100/782], Loss: 0.4145\n",
            "Epoch [34/50], Step [200/782], Loss: 0.3130\n",
            "Epoch [34/50], Step [300/782], Loss: 0.5654\n",
            "Epoch [34/50], Step [400/782], Loss: 0.4139\n",
            "Epoch [34/50], Step [500/782], Loss: 0.7627\n",
            "Epoch [34/50], Step [600/782], Loss: 0.4008\n",
            "Epoch [34/50], Step [700/782], Loss: 0.4571\n",
            "Epoch [34/50], Average Loss: 0.4976\n",
            "Epoch [35/50], Step [100/782], Loss: 0.4174\n",
            "Epoch [35/50], Step [200/782], Loss: 0.5159\n",
            "Epoch [35/50], Step [300/782], Loss: 0.5683\n",
            "Epoch [35/50], Step [400/782], Loss: 0.4503\n",
            "Epoch [35/50], Step [500/782], Loss: 0.5218\n",
            "Epoch [35/50], Step [600/782], Loss: 0.5628\n",
            "Epoch [35/50], Step [700/782], Loss: 0.7293\n",
            "Epoch [35/50], Average Loss: 0.4986\n",
            "Epoch [36/50], Step [100/782], Loss: 0.4164\n",
            "Epoch [36/50], Step [200/782], Loss: 0.4423\n",
            "Epoch [36/50], Step [300/782], Loss: 0.7498\n",
            "Epoch [36/50], Step [400/782], Loss: 0.4614\n",
            "Epoch [36/50], Step [500/782], Loss: 0.5051\n",
            "Epoch [36/50], Step [600/782], Loss: 0.4711\n",
            "Epoch [36/50], Step [700/782], Loss: 0.5452\n",
            "Epoch [36/50], Average Loss: 0.4989\n",
            "Epoch [37/50], Step [100/782], Loss: 0.4470\n",
            "Epoch [37/50], Step [200/782], Loss: 0.5546\n",
            "Epoch [37/50], Step [300/782], Loss: 0.2962\n",
            "Epoch [37/50], Step [400/782], Loss: 0.6349\n",
            "Epoch [37/50], Step [500/782], Loss: 0.7180\n",
            "Epoch [37/50], Step [600/782], Loss: 0.3471\n",
            "Epoch [37/50], Step [700/782], Loss: 0.4088\n",
            "Epoch [37/50], Average Loss: 0.4959\n",
            "Epoch [38/50], Step [100/782], Loss: 0.5865\n",
            "Epoch [38/50], Step [200/782], Loss: 0.4242\n",
            "Epoch [38/50], Step [300/782], Loss: 0.3385\n",
            "Epoch [38/50], Step [400/782], Loss: 0.4385\n",
            "Epoch [38/50], Step [500/782], Loss: 0.4230\n",
            "Epoch [38/50], Step [600/782], Loss: 0.3927\n",
            "Epoch [38/50], Step [700/782], Loss: 0.4688\n",
            "Epoch [38/50], Average Loss: 0.4976\n",
            "Epoch [39/50], Step [100/782], Loss: 0.5929\n",
            "Epoch [39/50], Step [200/782], Loss: 0.4664\n",
            "Epoch [39/50], Step [300/782], Loss: 0.2579\n",
            "Epoch [39/50], Step [400/782], Loss: 0.4361\n",
            "Epoch [39/50], Step [500/782], Loss: 0.4445\n",
            "Epoch [39/50], Step [600/782], Loss: 0.5908\n",
            "Epoch [39/50], Step [700/782], Loss: 0.5084\n",
            "Epoch [39/50], Average Loss: 0.4938\n",
            "Epoch [40/50], Step [100/782], Loss: 0.5932\n",
            "Epoch [40/50], Step [200/782], Loss: 0.5107\n",
            "Epoch [40/50], Step [300/782], Loss: 0.5900\n",
            "Epoch [40/50], Step [400/782], Loss: 0.5548\n",
            "Epoch [40/50], Step [500/782], Loss: 0.3792\n",
            "Epoch [40/50], Step [600/782], Loss: 0.6084\n",
            "Epoch [40/50], Step [700/782], Loss: 0.6624\n",
            "Epoch [40/50], Average Loss: 0.4908\n",
            "Epoch [41/50], Step [100/782], Loss: 0.3765\n",
            "Epoch [41/50], Step [200/782], Loss: 0.5149\n",
            "Epoch [41/50], Step [300/782], Loss: 0.5790\n",
            "Epoch [41/50], Step [400/782], Loss: 0.3269\n",
            "Epoch [41/50], Step [500/782], Loss: 0.4971\n",
            "Epoch [41/50], Step [600/782], Loss: 0.6345\n",
            "Epoch [41/50], Step [700/782], Loss: 0.6003\n",
            "Epoch [41/50], Average Loss: 0.4888\n",
            "Epoch [42/50], Step [100/782], Loss: 0.3637\n",
            "Epoch [42/50], Step [200/782], Loss: 0.7996\n",
            "Epoch [42/50], Step [300/782], Loss: 0.5175\n",
            "Epoch [42/50], Step [400/782], Loss: 0.4871\n",
            "Epoch [42/50], Step [500/782], Loss: 0.5491\n",
            "Epoch [42/50], Step [600/782], Loss: 0.3789\n",
            "Epoch [42/50], Step [700/782], Loss: 0.5188\n",
            "Epoch [42/50], Average Loss: 0.4835\n",
            "Epoch [43/50], Step [100/782], Loss: 0.5978\n",
            "Epoch [43/50], Step [200/782], Loss: 0.2667\n",
            "Epoch [43/50], Step [300/782], Loss: 0.6548\n",
            "Epoch [43/50], Step [400/782], Loss: 0.4690\n",
            "Epoch [43/50], Step [500/782], Loss: 0.3773\n",
            "Epoch [43/50], Step [600/782], Loss: 0.5797\n",
            "Epoch [43/50], Step [700/782], Loss: 0.3624\n",
            "Epoch [43/50], Average Loss: 0.4862\n",
            "Epoch [44/50], Step [100/782], Loss: 0.6529\n",
            "Epoch [44/50], Step [200/782], Loss: 0.4760\n",
            "Epoch [44/50], Step [300/782], Loss: 0.5318\n",
            "Epoch [44/50], Step [400/782], Loss: 0.8221\n",
            "Epoch [44/50], Step [500/782], Loss: 0.5606\n",
            "Epoch [44/50], Step [600/782], Loss: 0.4413\n",
            "Epoch [44/50], Step [700/782], Loss: 0.5393\n",
            "Epoch [44/50], Average Loss: 0.4828\n",
            "Epoch [45/50], Step [100/782], Loss: 0.3999\n",
            "Epoch [45/50], Step [200/782], Loss: 0.3995\n",
            "Epoch [45/50], Step [300/782], Loss: 0.5070\n",
            "Epoch [45/50], Step [400/782], Loss: 0.5730\n",
            "Epoch [45/50], Step [500/782], Loss: 0.5468\n",
            "Epoch [45/50], Step [600/782], Loss: 0.5324\n",
            "Epoch [45/50], Step [700/782], Loss: 0.4051\n",
            "Epoch [45/50], Average Loss: 0.4855\n",
            "Epoch [46/50], Step [100/782], Loss: 0.5150\n",
            "Epoch [46/50], Step [200/782], Loss: 0.4818\n",
            "Epoch [46/50], Step [300/782], Loss: 0.6050\n",
            "Epoch [46/50], Step [400/782], Loss: 0.8678\n",
            "Epoch [46/50], Step [500/782], Loss: 0.5632\n",
            "Epoch [46/50], Step [600/782], Loss: 0.5444\n",
            "Epoch [46/50], Step [700/782], Loss: 0.3734\n",
            "Epoch [46/50], Average Loss: 0.4846\n",
            "Epoch [47/50], Step [100/782], Loss: 0.4747\n",
            "Epoch [47/50], Step [200/782], Loss: 0.5246\n",
            "Epoch [47/50], Step [300/782], Loss: 0.7450\n",
            "Epoch [47/50], Step [400/782], Loss: 0.3413\n",
            "Epoch [47/50], Step [500/782], Loss: 0.3278\n",
            "Epoch [47/50], Step [600/782], Loss: 0.4851\n",
            "Epoch [47/50], Step [700/782], Loss: 0.5806\n",
            "Epoch [47/50], Average Loss: 0.4828\n",
            "Epoch [48/50], Step [100/782], Loss: 0.4924\n",
            "Epoch [48/50], Step [200/782], Loss: 0.4868\n",
            "Epoch [48/50], Step [300/782], Loss: 0.3043\n",
            "Epoch [48/50], Step [400/782], Loss: 0.6071\n",
            "Epoch [48/50], Step [500/782], Loss: 0.6543\n",
            "Epoch [48/50], Step [600/782], Loss: 0.5968\n",
            "Epoch [48/50], Step [700/782], Loss: 0.5436\n",
            "Epoch [48/50], Average Loss: 0.4813\n",
            "Epoch [49/50], Step [100/782], Loss: 0.6417\n",
            "Epoch [49/50], Step [200/782], Loss: 0.4607\n",
            "Epoch [49/50], Step [300/782], Loss: 0.7409\n",
            "Epoch [49/50], Step [400/782], Loss: 0.3362\n",
            "Epoch [49/50], Step [500/782], Loss: 0.2964\n",
            "Epoch [49/50], Step [600/782], Loss: 0.4382\n",
            "Epoch [49/50], Step [700/782], Loss: 0.4959\n",
            "Epoch [49/50], Average Loss: 0.4848\n",
            "Epoch [50/50], Step [100/782], Loss: 0.4519\n",
            "Epoch [50/50], Step [200/782], Loss: 0.5315\n",
            "Epoch [50/50], Step [300/782], Loss: 0.4463\n",
            "Epoch [50/50], Step [400/782], Loss: 0.4447\n",
            "Epoch [50/50], Step [500/782], Loss: 0.4286\n",
            "Epoch [50/50], Step [600/782], Loss: 0.5629\n",
            "Epoch [50/50], Step [700/782], Loss: 0.4962\n",
            "Epoch [50/50], Average Loss: 0.4765\n",
            "Accuracy of the model on the test images: 80.73 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yDBLpiCrViBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "44AG0OmEViMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git init\n",
        "!git add .\n",
        "!git config --global user.email \"victor.nitk@gmail.com\"\n",
        "!git config --global user.name \"Victor Daniel\"\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tODGCB2iLn24",
        "outputId": "d184eaf6-2aab-46bb-ef5e-eede94cece30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reinitialized existing Git repository in /content/.git/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git commit -m \"Initial commit\"\n",
        "!git remote add origin https://github.com/victordaniel/DEEP-LEARNIG-COURSE.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSWrwDsqL--c",
        "outputId": "54b402c6-3c3f-4342-9812-d4f2b0e2f28e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch master\n",
            "nothing to commit, working tree clean\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git push -u origin main\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1T4DpxbYMTlH",
        "outputId": "92861a74-2e7c-47d1-dfa0-32ff938ba267"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error: src refspec main does not match any\n",
            "\u001b[31merror: failed to push some refs to 'https://github.com/victordaniel/DEEP-LEARNIG-COURSE.git'\n",
            "\u001b[m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IDpxD_MWVkua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#BASE MODEL WITH TIME AND ACCURACY PRINTED"
      ],
      "metadata": {
        "id": "ZJdd6839Vlpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Function to save the model\n",
        "def save_model(epoch, model, optimizer, loss, path):\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss,\n",
        "    }, path)\n",
        "\n",
        "# Function to load the model\n",
        "def load_model(path, model, optimizer):\n",
        "    checkpoint = torch.load(path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    epoch = checkpoint['epoch']\n",
        "    loss = checkpoint['loss']\n",
        "    return epoch, loss\n",
        "\n",
        "# Function to train the model\n",
        "def train_model(model, train_loader, criterion, optimizer, scheduler, num_epochs=50):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_train_loss = 0.0\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_train_loss += loss.item()\n",
        "            if (i+1) % 100 == 0:\n",
        "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "        epoch_train_loss = running_train_loss / len(train_loader)\n",
        "        train_losses.append(epoch_train_loss)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {epoch_train_loss:.4f}')\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        running_val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in test_loader:\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                running_val_loss += loss.item()\n",
        "\n",
        "        epoch_val_loss = running_val_loss / len(test_loader)\n",
        "        val_losses.append(epoch_val_loss)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {epoch_val_loss:.4f}')\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Save the model if the validation loss is the best we've seen so far.\n",
        "        if epoch_val_loss < best_loss:\n",
        "            best_loss = epoch_val_loss\n",
        "            save_model(epoch, model, optimizer, epoch_val_loss, './model.pth')\n",
        "\n",
        "    return train_losses, val_losses\n",
        "\n",
        "# Function to evaluate the model\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Define the transform with data augmentation for advanced models\n",
        "transform_advanced = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Define the transform without data augmentation for the baseline model\n",
        "transform_baseline = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load the datasets\n",
        "train_dataset_baseline = CIFAR10(root='./data', train=True, download=True, transform=transform_baseline)\n",
        "test_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform_baseline)\n",
        "\n",
        "train_loader_baseline = DataLoader(dataset=train_dataset_baseline, batch_size=64, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "train_dataset_advanced = CIFAR10(root='./data', train=True, download=True, transform=transform_advanced)\n",
        "train_loader_advanced = DataLoader(dataset=train_dataset_advanced, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "# Baseline Model\n",
        "class BaselineCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BaselineCNN, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2))\n",
        "        self.fc1 = nn.Linear(32 * 8 * 8, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "# Model 1\n",
        "class Model1CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model1CNN, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2))\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2))\n",
        "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc1(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "# Model 2\n",
        "class Model2CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model2CNN, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2))\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2))\n",
        "        self.fc1 = nn.Linear(256 * 4 * 4, 512)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc1(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "# Training and evaluation for Baseline Model\n",
        "model_baseline = BaselineCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_baseline.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
        "\n",
        "start_time = time.time()\n",
        "train_losses, val_losses = train_model(model_baseline, train_loader_baseline, criterion, optimizer, scheduler, num_epochs=50)\n",
        "end_time = time.time()\n",
        "time_baseline = end_time - start_time\n",
        "accuracy_baseline = evaluate_model(model_baseline, test_loader)\n",
        "print(f'Baseline Model - Time: {time_baseline:.2f}s, Accuracy: {accuracy_baseline:.2f}%')\n",
        "\n",
        "# Training and evaluation for Model 1\n",
        "model1 = Model1CNN().to(device)\n",
        "optimizer = optim.Adam(model1.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(train_loader_advanced), epochs=50)\n",
        "\n",
        "start_time = time.time()\n",
        "train_losses, val_losses = train_model(model1, train_loader_advanced, criterion, optimizer, scheduler, num_epochs=50)\n",
        "end_time = time.time()\n",
        "time_model1 = end_time - start_time\n",
        "accuracy_model1 = evaluate_model(model1, test_loader)\n",
        "print(f'Model 1 - Time: {time_model1:.2f}s, Accuracy: {accuracy_model1:.2f}%')\n",
        "\n",
        "# Training and evaluation for Model 2\n",
        "model2 = Model2CNN().to(device)\n",
        "optimizer = optim.Adam(model2.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(train_loader_advanced), epochs=50)\n",
        "\n",
        "start_time = time.time()\n",
        "train_losses, val_losses = train_model(model2, train_loader_advanced, criterion, optimizer, scheduler, num_epochs=50)\n",
        "end_time = time.time()\n",
        "time_model2 = end_time - start_time\n",
        "accuracy_model2 = evaluate_model(model2, test_loader)\n",
        "print(f'Model 2 - Time: {time_model2:.2f}s, Accuracy: {accuracy_model2:.2f}%')\n",
        "\n",
        "# Summary\n",
        "print(f'Baseline Model - Time: {time_baseline:.2f}s, Accuracy: {accuracy_baseline:.2f}%')\n",
        "print(f'Model 1 - Time: {time_model1:.2f}s, Accuracy: {accuracy_model1:.2f}%')\n",
        "print(f'Model 2 - Time: {time_model2:.2f}s, Accuracy: {accuracy_model2:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T88wdk3QVrrP",
        "outputId": "4dc0b9aa-b90e-46d4-a7b3-6c71d59c13ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch [1/50], Step [100/782], Loss: 1.6827\n",
            "Epoch [1/50], Step [200/782], Loss: 1.3844\n",
            "Epoch [1/50], Step [300/782], Loss: 1.4897\n",
            "Epoch [1/50], Step [400/782], Loss: 1.0379\n",
            "Epoch [1/50], Step [500/782], Loss: 1.1358\n",
            "Epoch [1/50], Step [600/782], Loss: 1.6833\n",
            "Epoch [1/50], Step [700/782], Loss: 1.1050\n",
            "Epoch [1/50], Training Loss: 1.3358\n",
            "Epoch [1/50], Validation Loss: 1.1732\n",
            "Epoch [2/50], Step [100/782], Loss: 0.9704\n",
            "Epoch [2/50], Step [200/782], Loss: 0.8630\n",
            "Epoch [2/50], Step [300/782], Loss: 0.8133\n",
            "Epoch [2/50], Step [400/782], Loss: 1.1475\n",
            "Epoch [2/50], Step [500/782], Loss: 0.7859\n",
            "Epoch [2/50], Step [600/782], Loss: 0.9721\n",
            "Epoch [2/50], Step [700/782], Loss: 1.0769\n",
            "Epoch [2/50], Training Loss: 1.0147\n",
            "Epoch [2/50], Validation Loss: 0.9670\n",
            "Epoch [3/50], Step [100/782], Loss: 0.7864\n",
            "Epoch [3/50], Step [200/782], Loss: 1.0399\n",
            "Epoch [3/50], Step [300/782], Loss: 1.1820\n",
            "Epoch [3/50], Step [400/782], Loss: 0.9553\n",
            "Epoch [3/50], Step [500/782], Loss: 1.0774\n",
            "Epoch [3/50], Step [600/782], Loss: 0.8578\n",
            "Epoch [3/50], Step [700/782], Loss: 0.5938\n",
            "Epoch [3/50], Training Loss: 0.9147\n",
            "Epoch [3/50], Validation Loss: 0.9117\n",
            "Epoch [4/50], Step [100/782], Loss: 0.6813\n",
            "Epoch [4/50], Step [200/782], Loss: 0.6306\n",
            "Epoch [4/50], Step [300/782], Loss: 0.9539\n",
            "Epoch [4/50], Step [400/782], Loss: 0.8859\n",
            "Epoch [4/50], Step [500/782], Loss: 0.9462\n",
            "Epoch [4/50], Step [600/782], Loss: 0.6491\n",
            "Epoch [4/50], Step [700/782], Loss: 0.7445\n",
            "Epoch [4/50], Training Loss: 0.8545\n",
            "Epoch [4/50], Validation Loss: 0.9524\n",
            "Epoch [5/50], Step [100/782], Loss: 0.7787\n",
            "Epoch [5/50], Step [200/782], Loss: 0.6061\n",
            "Epoch [5/50], Step [300/782], Loss: 0.8332\n",
            "Epoch [5/50], Step [400/782], Loss: 0.8630\n",
            "Epoch [5/50], Step [500/782], Loss: 0.7456\n",
            "Epoch [5/50], Step [600/782], Loss: 0.7468\n",
            "Epoch [5/50], Step [700/782], Loss: 0.7646\n",
            "Epoch [5/50], Training Loss: 0.8173\n",
            "Epoch [5/50], Validation Loss: 0.8704\n",
            "Epoch [6/50], Step [100/782], Loss: 0.5631\n",
            "Epoch [6/50], Step [200/782], Loss: 0.8734\n",
            "Epoch [6/50], Step [300/782], Loss: 0.6163\n",
            "Epoch [6/50], Step [400/782], Loss: 0.8849\n",
            "Epoch [6/50], Step [500/782], Loss: 0.7031\n",
            "Epoch [6/50], Step [600/782], Loss: 0.8193\n",
            "Epoch [6/50], Step [700/782], Loss: 0.9848\n",
            "Epoch [6/50], Training Loss: 0.7809\n",
            "Epoch [6/50], Validation Loss: 0.8851\n",
            "Epoch [7/50], Step [100/782], Loss: 0.8145\n",
            "Epoch [7/50], Step [200/782], Loss: 0.7569\n",
            "Epoch [7/50], Step [300/782], Loss: 0.7762\n",
            "Epoch [7/50], Step [400/782], Loss: 0.7905\n",
            "Epoch [7/50], Step [500/782], Loss: 0.7466\n",
            "Epoch [7/50], Step [600/782], Loss: 0.5415\n",
            "Epoch [7/50], Step [700/782], Loss: 0.8032\n",
            "Epoch [7/50], Training Loss: 0.7560\n",
            "Epoch [7/50], Validation Loss: 0.8638\n",
            "Epoch [8/50], Step [100/782], Loss: 0.6754\n",
            "Epoch [8/50], Step [200/782], Loss: 0.6175\n",
            "Epoch [8/50], Step [300/782], Loss: 0.7133\n",
            "Epoch [8/50], Step [400/782], Loss: 0.6552\n",
            "Epoch [8/50], Step [500/782], Loss: 0.9062\n",
            "Epoch [8/50], Step [600/782], Loss: 0.8648\n",
            "Epoch [8/50], Step [700/782], Loss: 0.7826\n",
            "Epoch [8/50], Training Loss: 0.7331\n",
            "Epoch [8/50], Validation Loss: 0.8661\n",
            "Epoch [9/50], Step [100/782], Loss: 0.6856\n",
            "Epoch [9/50], Step [200/782], Loss: 0.5490\n",
            "Epoch [9/50], Step [300/782], Loss: 0.8804\n",
            "Epoch [9/50], Step [400/782], Loss: 0.8514\n",
            "Epoch [9/50], Step [500/782], Loss: 0.6421\n",
            "Epoch [9/50], Step [600/782], Loss: 0.6233\n",
            "Epoch [9/50], Step [700/782], Loss: 0.4503\n",
            "Epoch [9/50], Training Loss: 0.7124\n",
            "Epoch [9/50], Validation Loss: 0.8616\n",
            "Epoch [10/50], Step [100/782], Loss: 0.9180\n",
            "Epoch [10/50], Step [200/782], Loss: 0.5822\n",
            "Epoch [10/50], Step [300/782], Loss: 0.6508\n",
            "Epoch [10/50], Step [400/782], Loss: 0.9280\n",
            "Epoch [10/50], Step [500/782], Loss: 0.8689\n",
            "Epoch [10/50], Step [600/782], Loss: 0.8317\n",
            "Epoch [10/50], Step [700/782], Loss: 0.8521\n",
            "Epoch [10/50], Training Loss: 0.6912\n",
            "Epoch [10/50], Validation Loss: 0.8601\n",
            "Epoch [11/50], Step [100/782], Loss: 0.6138\n",
            "Epoch [11/50], Step [200/782], Loss: 0.5407\n",
            "Epoch [11/50], Step [300/782], Loss: 0.6357\n",
            "Epoch [11/50], Step [400/782], Loss: 0.3058\n",
            "Epoch [11/50], Step [500/782], Loss: 0.6802\n",
            "Epoch [11/50], Step [600/782], Loss: 0.8550\n",
            "Epoch [11/50], Step [700/782], Loss: 0.7318\n",
            "Epoch [11/50], Training Loss: 0.6736\n",
            "Epoch [11/50], Validation Loss: 0.8574\n",
            "Epoch [12/50], Step [100/782], Loss: 0.7020\n",
            "Epoch [12/50], Step [200/782], Loss: 0.5482\n",
            "Epoch [12/50], Step [300/782], Loss: 0.6334\n",
            "Epoch [12/50], Step [400/782], Loss: 0.4372\n",
            "Epoch [12/50], Step [500/782], Loss: 0.6028\n",
            "Epoch [12/50], Step [600/782], Loss: 0.7052\n",
            "Epoch [12/50], Step [700/782], Loss: 0.5646\n",
            "Epoch [12/50], Training Loss: 0.6620\n",
            "Epoch [12/50], Validation Loss: 0.9220\n",
            "Epoch [13/50], Step [100/782], Loss: 0.6652\n",
            "Epoch [13/50], Step [200/782], Loss: 0.6963\n",
            "Epoch [13/50], Step [300/782], Loss: 0.8404\n",
            "Epoch [13/50], Step [400/782], Loss: 0.6348\n",
            "Epoch [13/50], Step [500/782], Loss: 0.6063\n",
            "Epoch [13/50], Step [600/782], Loss: 0.7317\n",
            "Epoch [13/50], Step [700/782], Loss: 0.5370\n",
            "Epoch [13/50], Training Loss: 0.6429\n",
            "Epoch [13/50], Validation Loss: 0.8830\n",
            "Epoch [14/50], Step [100/782], Loss: 0.6799\n",
            "Epoch [14/50], Step [200/782], Loss: 0.5847\n",
            "Epoch [14/50], Step [300/782], Loss: 0.8015\n",
            "Epoch [14/50], Step [400/782], Loss: 0.5264\n",
            "Epoch [14/50], Step [500/782], Loss: 0.5332\n",
            "Epoch [14/50], Step [600/782], Loss: 0.5468\n",
            "Epoch [14/50], Step [700/782], Loss: 0.4465\n",
            "Epoch [14/50], Training Loss: 0.6341\n",
            "Epoch [14/50], Validation Loss: 0.8915\n",
            "Epoch [15/50], Step [100/782], Loss: 0.7596\n",
            "Epoch [15/50], Step [200/782], Loss: 0.8413\n",
            "Epoch [15/50], Step [300/782], Loss: 0.5243\n",
            "Epoch [15/50], Step [400/782], Loss: 0.8742\n",
            "Epoch [15/50], Step [500/782], Loss: 0.7662\n",
            "Epoch [15/50], Step [600/782], Loss: 0.5312\n",
            "Epoch [15/50], Step [700/782], Loss: 0.5856\n",
            "Epoch [15/50], Training Loss: 0.6245\n",
            "Epoch [15/50], Validation Loss: 0.9174\n",
            "Epoch [16/50], Step [100/782], Loss: 0.5538\n",
            "Epoch [16/50], Step [200/782], Loss: 0.7873\n",
            "Epoch [16/50], Step [300/782], Loss: 0.7615\n",
            "Epoch [16/50], Step [400/782], Loss: 0.2952\n",
            "Epoch [16/50], Step [500/782], Loss: 0.6659\n",
            "Epoch [16/50], Step [600/782], Loss: 0.4530\n",
            "Epoch [16/50], Step [700/782], Loss: 0.5997\n",
            "Epoch [16/50], Training Loss: 0.6120\n",
            "Epoch [16/50], Validation Loss: 0.8705\n",
            "Epoch [17/50], Step [100/782], Loss: 0.4994\n",
            "Epoch [17/50], Step [200/782], Loss: 0.6533\n",
            "Epoch [17/50], Step [300/782], Loss: 0.5668\n",
            "Epoch [17/50], Step [400/782], Loss: 0.7605\n",
            "Epoch [17/50], Step [500/782], Loss: 0.6938\n",
            "Epoch [17/50], Step [600/782], Loss: 0.8078\n",
            "Epoch [17/50], Step [700/782], Loss: 0.5353\n",
            "Epoch [17/50], Training Loss: 0.6008\n",
            "Epoch [17/50], Validation Loss: 0.8858\n",
            "Epoch [18/50], Step [100/782], Loss: 0.6508\n",
            "Epoch [18/50], Step [200/782], Loss: 0.5584\n",
            "Epoch [18/50], Step [300/782], Loss: 0.5054\n",
            "Epoch [18/50], Step [400/782], Loss: 0.7355\n",
            "Epoch [18/50], Step [500/782], Loss: 0.7016\n",
            "Epoch [18/50], Step [600/782], Loss: 0.5390\n",
            "Epoch [18/50], Step [700/782], Loss: 0.5588\n",
            "Epoch [18/50], Training Loss: 0.5932\n",
            "Epoch [18/50], Validation Loss: 0.9245\n",
            "Epoch [19/50], Step [100/782], Loss: 0.4841\n",
            "Epoch [19/50], Step [200/782], Loss: 0.6315\n",
            "Epoch [19/50], Step [300/782], Loss: 0.5259\n",
            "Epoch [19/50], Step [400/782], Loss: 0.8271\n",
            "Epoch [19/50], Step [500/782], Loss: 0.4423\n",
            "Epoch [19/50], Step [600/782], Loss: 0.5914\n",
            "Epoch [19/50], Step [700/782], Loss: 0.5233\n",
            "Epoch [19/50], Training Loss: 0.5872\n",
            "Epoch [19/50], Validation Loss: 0.9020\n",
            "Epoch [20/50], Step [100/782], Loss: 0.5144\n",
            "Epoch [20/50], Step [200/782], Loss: 0.5467\n",
            "Epoch [20/50], Step [300/782], Loss: 0.4329\n",
            "Epoch [20/50], Step [400/782], Loss: 0.5164\n",
            "Epoch [20/50], Step [500/782], Loss: 0.6780\n",
            "Epoch [20/50], Step [600/782], Loss: 0.5164\n",
            "Epoch [20/50], Step [700/782], Loss: 0.4093\n",
            "Epoch [20/50], Training Loss: 0.5825\n",
            "Epoch [20/50], Validation Loss: 0.9246\n",
            "Epoch [21/50], Step [100/782], Loss: 0.5029\n",
            "Epoch [21/50], Step [200/782], Loss: 0.7264\n",
            "Epoch [21/50], Step [300/782], Loss: 0.3517\n",
            "Epoch [21/50], Step [400/782], Loss: 0.4110\n",
            "Epoch [21/50], Step [500/782], Loss: 0.3694\n",
            "Epoch [21/50], Step [600/782], Loss: 0.3681\n",
            "Epoch [21/50], Step [700/782], Loss: 0.5583\n",
            "Epoch [21/50], Training Loss: 0.4746\n",
            "Epoch [21/50], Validation Loss: 0.8624\n",
            "Epoch [22/50], Step [100/782], Loss: 0.6827\n",
            "Epoch [22/50], Step [200/782], Loss: 0.4232\n",
            "Epoch [22/50], Step [300/782], Loss: 0.3641\n",
            "Epoch [22/50], Step [400/782], Loss: 0.5399\n",
            "Epoch [22/50], Step [500/782], Loss: 0.3621\n",
            "Epoch [22/50], Step [600/782], Loss: 0.3745\n",
            "Epoch [22/50], Step [700/782], Loss: 0.5401\n",
            "Epoch [22/50], Training Loss: 0.4586\n",
            "Epoch [22/50], Validation Loss: 0.8768\n",
            "Epoch [23/50], Step [100/782], Loss: 0.4483\n",
            "Epoch [23/50], Step [200/782], Loss: 0.4060\n",
            "Epoch [23/50], Step [300/782], Loss: 0.3086\n",
            "Epoch [23/50], Step [400/782], Loss: 0.6475\n",
            "Epoch [23/50], Step [500/782], Loss: 0.4403\n",
            "Epoch [23/50], Step [600/782], Loss: 0.4789\n",
            "Epoch [23/50], Step [700/782], Loss: 0.5032\n",
            "Epoch [23/50], Training Loss: 0.4509\n",
            "Epoch [23/50], Validation Loss: 0.8845\n",
            "Epoch [24/50], Step [100/782], Loss: 0.4188\n",
            "Epoch [24/50], Step [200/782], Loss: 0.5215\n",
            "Epoch [24/50], Step [300/782], Loss: 0.7480\n",
            "Epoch [24/50], Step [400/782], Loss: 0.3535\n",
            "Epoch [24/50], Step [500/782], Loss: 0.4690\n",
            "Epoch [24/50], Step [600/782], Loss: 0.4272\n",
            "Epoch [24/50], Step [700/782], Loss: 0.4291\n",
            "Epoch [24/50], Training Loss: 0.4451\n",
            "Epoch [24/50], Validation Loss: 0.9003\n",
            "Epoch [25/50], Step [100/782], Loss: 0.3613\n",
            "Epoch [25/50], Step [200/782], Loss: 0.4651\n",
            "Epoch [25/50], Step [300/782], Loss: 0.4254\n",
            "Epoch [25/50], Step [400/782], Loss: 0.3554\n",
            "Epoch [25/50], Step [500/782], Loss: 0.3661\n",
            "Epoch [25/50], Step [600/782], Loss: 0.3872\n",
            "Epoch [25/50], Step [700/782], Loss: 0.2246\n",
            "Epoch [25/50], Training Loss: 0.4397\n",
            "Epoch [25/50], Validation Loss: 0.9140\n",
            "Epoch [26/50], Step [100/782], Loss: 0.3581\n",
            "Epoch [26/50], Step [200/782], Loss: 0.6148\n",
            "Epoch [26/50], Step [300/782], Loss: 0.4062\n",
            "Epoch [26/50], Step [400/782], Loss: 0.6607\n",
            "Epoch [26/50], Step [500/782], Loss: 0.4182\n",
            "Epoch [26/50], Step [600/782], Loss: 0.3880\n",
            "Epoch [26/50], Step [700/782], Loss: 0.4031\n",
            "Epoch [26/50], Training Loss: 0.4353\n",
            "Epoch [26/50], Validation Loss: 0.9174\n",
            "Epoch [27/50], Step [100/782], Loss: 0.3654\n",
            "Epoch [27/50], Step [200/782], Loss: 0.5238\n",
            "Epoch [27/50], Step [300/782], Loss: 0.3000\n",
            "Epoch [27/50], Step [400/782], Loss: 0.4348\n",
            "Epoch [27/50], Step [500/782], Loss: 0.4989\n",
            "Epoch [27/50], Step [600/782], Loss: 0.4962\n",
            "Epoch [27/50], Step [700/782], Loss: 0.5173\n",
            "Epoch [27/50], Training Loss: 0.4337\n",
            "Epoch [27/50], Validation Loss: 0.9321\n",
            "Epoch [28/50], Step [100/782], Loss: 0.3906\n",
            "Epoch [28/50], Step [200/782], Loss: 0.4140\n",
            "Epoch [28/50], Step [300/782], Loss: 0.5032\n",
            "Epoch [28/50], Step [400/782], Loss: 0.4352\n",
            "Epoch [28/50], Step [500/782], Loss: 0.2357\n",
            "Epoch [28/50], Step [600/782], Loss: 0.3653\n",
            "Epoch [28/50], Step [700/782], Loss: 0.4471\n",
            "Epoch [28/50], Training Loss: 0.4301\n",
            "Epoch [28/50], Validation Loss: 0.9340\n",
            "Epoch [29/50], Step [100/782], Loss: 0.4372\n",
            "Epoch [29/50], Step [200/782], Loss: 0.4560\n",
            "Epoch [29/50], Step [300/782], Loss: 0.4707\n",
            "Epoch [29/50], Step [400/782], Loss: 0.4645\n",
            "Epoch [29/50], Step [500/782], Loss: 0.4944\n",
            "Epoch [29/50], Step [600/782], Loss: 0.3922\n",
            "Epoch [29/50], Step [700/782], Loss: 0.3337\n",
            "Epoch [29/50], Training Loss: 0.4272\n",
            "Epoch [29/50], Validation Loss: 0.9430\n",
            "Epoch [30/50], Step [100/782], Loss: 0.4159\n",
            "Epoch [30/50], Step [200/782], Loss: 0.2933\n",
            "Epoch [30/50], Step [300/782], Loss: 0.4898\n",
            "Epoch [30/50], Step [400/782], Loss: 0.2933\n",
            "Epoch [30/50], Step [500/782], Loss: 0.4656\n",
            "Epoch [30/50], Step [600/782], Loss: 0.4584\n",
            "Epoch [30/50], Step [700/782], Loss: 0.3795\n",
            "Epoch [30/50], Training Loss: 0.4249\n",
            "Epoch [30/50], Validation Loss: 0.9560\n",
            "Epoch [31/50], Step [100/782], Loss: 0.3302\n",
            "Epoch [31/50], Step [200/782], Loss: 0.3500\n",
            "Epoch [31/50], Step [300/782], Loss: 0.5017\n",
            "Epoch [31/50], Step [400/782], Loss: 0.5271\n",
            "Epoch [31/50], Step [500/782], Loss: 0.3396\n",
            "Epoch [31/50], Step [600/782], Loss: 0.4092\n",
            "Epoch [31/50], Step [700/782], Loss: 0.5292\n",
            "Epoch [31/50], Training Loss: 0.4226\n",
            "Epoch [31/50], Validation Loss: 0.9550\n",
            "Epoch [32/50], Step [100/782], Loss: 0.3119\n",
            "Epoch [32/50], Step [200/782], Loss: 0.3100\n",
            "Epoch [32/50], Step [300/782], Loss: 0.5025\n",
            "Epoch [32/50], Step [400/782], Loss: 0.5241\n",
            "Epoch [32/50], Step [500/782], Loss: 0.4931\n",
            "Epoch [32/50], Step [600/782], Loss: 0.4249\n",
            "Epoch [32/50], Step [700/782], Loss: 0.4979\n",
            "Epoch [32/50], Training Loss: 0.4210\n",
            "Epoch [32/50], Validation Loss: 0.9703\n",
            "Epoch [33/50], Step [100/782], Loss: 0.3999\n",
            "Epoch [33/50], Step [200/782], Loss: 0.4190\n",
            "Epoch [33/50], Step [300/782], Loss: 0.3680\n",
            "Epoch [33/50], Step [400/782], Loss: 0.4050\n",
            "Epoch [33/50], Step [500/782], Loss: 0.4551\n",
            "Epoch [33/50], Step [600/782], Loss: 0.4212\n",
            "Epoch [33/50], Step [700/782], Loss: 0.3909\n",
            "Epoch [33/50], Training Loss: 0.4177\n",
            "Epoch [33/50], Validation Loss: 0.9649\n",
            "Epoch [34/50], Step [100/782], Loss: 0.5475\n",
            "Epoch [34/50], Step [200/782], Loss: 0.4853\n",
            "Epoch [34/50], Step [300/782], Loss: 0.4993\n",
            "Epoch [34/50], Step [400/782], Loss: 0.6079\n",
            "Epoch [34/50], Step [500/782], Loss: 0.5519\n",
            "Epoch [34/50], Step [600/782], Loss: 0.4723\n",
            "Epoch [34/50], Step [700/782], Loss: 0.2915\n",
            "Epoch [34/50], Training Loss: 0.4154\n",
            "Epoch [34/50], Validation Loss: 0.9911\n",
            "Epoch [35/50], Step [100/782], Loss: 0.3045\n",
            "Epoch [35/50], Step [200/782], Loss: 0.2407\n",
            "Epoch [35/50], Step [300/782], Loss: 0.3451\n",
            "Epoch [35/50], Step [400/782], Loss: 0.5405\n",
            "Epoch [35/50], Step [500/782], Loss: 0.5178\n",
            "Epoch [35/50], Step [600/782], Loss: 0.2581\n",
            "Epoch [35/50], Step [700/782], Loss: 0.3810\n",
            "Epoch [35/50], Training Loss: 0.4145\n",
            "Epoch [35/50], Validation Loss: 0.9854\n",
            "Epoch [36/50], Step [100/782], Loss: 0.4793\n",
            "Epoch [36/50], Step [200/782], Loss: 0.2357\n",
            "Epoch [36/50], Step [300/782], Loss: 0.3460\n",
            "Epoch [36/50], Step [400/782], Loss: 0.4737\n",
            "Epoch [36/50], Step [500/782], Loss: 0.3403\n",
            "Epoch [36/50], Step [600/782], Loss: 0.5017\n",
            "Epoch [36/50], Step [700/782], Loss: 0.4088\n",
            "Epoch [36/50], Training Loss: 0.4120\n",
            "Epoch [36/50], Validation Loss: 0.9947\n",
            "Epoch [37/50], Step [100/782], Loss: 0.3350\n",
            "Epoch [37/50], Step [200/782], Loss: 0.3635\n",
            "Epoch [37/50], Step [300/782], Loss: 0.5085\n",
            "Epoch [37/50], Step [400/782], Loss: 0.3683\n",
            "Epoch [37/50], Step [500/782], Loss: 0.3087\n",
            "Epoch [37/50], Step [600/782], Loss: 0.4208\n",
            "Epoch [37/50], Step [700/782], Loss: 0.3005\n",
            "Epoch [37/50], Training Loss: 0.4095\n",
            "Epoch [37/50], Validation Loss: 0.9942\n",
            "Epoch [38/50], Step [100/782], Loss: 0.3121\n",
            "Epoch [38/50], Step [200/782], Loss: 0.3526\n",
            "Epoch [38/50], Step [300/782], Loss: 0.5045\n",
            "Epoch [38/50], Step [400/782], Loss: 0.3530\n",
            "Epoch [38/50], Step [500/782], Loss: 0.2214\n",
            "Epoch [38/50], Step [600/782], Loss: 0.4124\n",
            "Epoch [38/50], Step [700/782], Loss: 0.5092\n",
            "Epoch [38/50], Training Loss: 0.4078\n",
            "Epoch [38/50], Validation Loss: 1.0010\n",
            "Epoch [39/50], Step [100/782], Loss: 0.3186\n",
            "Epoch [39/50], Step [200/782], Loss: 0.4485\n",
            "Epoch [39/50], Step [300/782], Loss: 0.4623\n",
            "Epoch [39/50], Step [400/782], Loss: 0.4043\n",
            "Epoch [39/50], Step [500/782], Loss: 0.2622\n",
            "Epoch [39/50], Step [600/782], Loss: 0.5518\n",
            "Epoch [39/50], Step [700/782], Loss: 0.2314\n",
            "Epoch [39/50], Training Loss: 0.4072\n",
            "Epoch [39/50], Validation Loss: 0.9973\n",
            "Epoch [40/50], Step [100/782], Loss: 0.4592\n",
            "Epoch [40/50], Step [200/782], Loss: 0.4801\n",
            "Epoch [40/50], Step [300/782], Loss: 0.3861\n",
            "Epoch [40/50], Step [400/782], Loss: 0.4782\n",
            "Epoch [40/50], Step [500/782], Loss: 0.3487\n",
            "Epoch [40/50], Step [600/782], Loss: 0.6490\n",
            "Epoch [40/50], Step [700/782], Loss: 0.5801\n",
            "Epoch [40/50], Training Loss: 0.4055\n",
            "Epoch [40/50], Validation Loss: 1.0150\n",
            "Epoch [41/50], Step [100/782], Loss: 0.3695\n",
            "Epoch [41/50], Step [200/782], Loss: 0.2654\n",
            "Epoch [41/50], Step [300/782], Loss: 0.3707\n",
            "Epoch [41/50], Step [400/782], Loss: 0.4316\n",
            "Epoch [41/50], Step [500/782], Loss: 0.3206\n",
            "Epoch [41/50], Step [600/782], Loss: 0.4980\n",
            "Epoch [41/50], Step [700/782], Loss: 0.4297\n",
            "Epoch [41/50], Training Loss: 0.3884\n",
            "Epoch [41/50], Validation Loss: 1.0167\n",
            "Epoch [42/50], Step [100/782], Loss: 0.2138\n",
            "Epoch [42/50], Step [200/782], Loss: 0.4803\n",
            "Epoch [42/50], Step [300/782], Loss: 0.5040\n",
            "Epoch [42/50], Step [400/782], Loss: 0.4126\n",
            "Epoch [42/50], Step [500/782], Loss: 0.2372\n",
            "Epoch [42/50], Step [600/782], Loss: 0.6346\n",
            "Epoch [42/50], Step [700/782], Loss: 0.4775\n",
            "Epoch [42/50], Training Loss: 0.3863\n",
            "Epoch [42/50], Validation Loss: 1.0063\n",
            "Epoch [43/50], Step [100/782], Loss: 0.3160\n",
            "Epoch [43/50], Step [200/782], Loss: 0.2473\n",
            "Epoch [43/50], Step [300/782], Loss: 0.3490\n",
            "Epoch [43/50], Step [400/782], Loss: 0.3993\n",
            "Epoch [43/50], Step [500/782], Loss: 0.4351\n",
            "Epoch [43/50], Step [600/782], Loss: 0.2377\n",
            "Epoch [43/50], Step [700/782], Loss: 0.4761\n",
            "Epoch [43/50], Training Loss: 0.3873\n",
            "Epoch [43/50], Validation Loss: 1.0021\n",
            "Epoch [44/50], Step [100/782], Loss: 0.2320\n",
            "Epoch [44/50], Step [200/782], Loss: 0.2852\n",
            "Epoch [44/50], Step [300/782], Loss: 0.4757\n",
            "Epoch [44/50], Step [400/782], Loss: 0.3720\n",
            "Epoch [44/50], Step [500/782], Loss: 0.3574\n",
            "Epoch [44/50], Step [600/782], Loss: 0.4269\n",
            "Epoch [44/50], Step [700/782], Loss: 0.4064\n",
            "Epoch [44/50], Training Loss: 0.3860\n",
            "Epoch [44/50], Validation Loss: 1.0213\n",
            "Epoch [45/50], Step [100/782], Loss: 0.3162\n",
            "Epoch [45/50], Step [200/782], Loss: 0.4927\n",
            "Epoch [45/50], Step [300/782], Loss: 0.4721\n",
            "Epoch [45/50], Step [400/782], Loss: 0.4262\n",
            "Epoch [45/50], Step [500/782], Loss: 0.2748\n",
            "Epoch [45/50], Step [600/782], Loss: 0.4708\n",
            "Epoch [45/50], Step [700/782], Loss: 0.2480\n",
            "Epoch [45/50], Training Loss: 0.3856\n",
            "Epoch [45/50], Validation Loss: 1.0072\n",
            "Epoch [46/50], Step [100/782], Loss: 0.4922\n",
            "Epoch [46/50], Step [200/782], Loss: 0.4822\n",
            "Epoch [46/50], Step [300/782], Loss: 0.3689\n",
            "Epoch [46/50], Step [400/782], Loss: 0.3058\n",
            "Epoch [46/50], Step [500/782], Loss: 0.3447\n",
            "Epoch [46/50], Step [600/782], Loss: 0.4367\n",
            "Epoch [46/50], Step [700/782], Loss: 0.6789\n",
            "Epoch [46/50], Training Loss: 0.3864\n",
            "Epoch [46/50], Validation Loss: 1.0060\n",
            "Epoch [47/50], Step [100/782], Loss: 0.3667\n",
            "Epoch [47/50], Step [200/782], Loss: 0.3301\n",
            "Epoch [47/50], Step [300/782], Loss: 0.3538\n",
            "Epoch [47/50], Step [400/782], Loss: 0.3410\n",
            "Epoch [47/50], Step [500/782], Loss: 0.4504\n",
            "Epoch [47/50], Step [600/782], Loss: 0.3531\n",
            "Epoch [47/50], Step [700/782], Loss: 0.4040\n",
            "Epoch [47/50], Training Loss: 0.3834\n",
            "Epoch [47/50], Validation Loss: 1.0187\n",
            "Epoch [48/50], Step [100/782], Loss: 0.3765\n",
            "Epoch [48/50], Step [200/782], Loss: 0.3679\n",
            "Epoch [48/50], Step [300/782], Loss: 0.3810\n",
            "Epoch [48/50], Step [400/782], Loss: 0.2042\n",
            "Epoch [48/50], Step [500/782], Loss: 0.3712\n",
            "Epoch [48/50], Step [600/782], Loss: 0.4380\n",
            "Epoch [48/50], Step [700/782], Loss: 0.3672\n",
            "Epoch [48/50], Training Loss: 0.3862\n",
            "Epoch [48/50], Validation Loss: 1.0197\n",
            "Epoch [49/50], Step [100/782], Loss: 0.3100\n",
            "Epoch [49/50], Step [200/782], Loss: 0.2354\n",
            "Epoch [49/50], Step [300/782], Loss: 0.3090\n",
            "Epoch [49/50], Step [400/782], Loss: 0.4029\n",
            "Epoch [49/50], Step [500/782], Loss: 0.3579\n",
            "Epoch [49/50], Step [600/782], Loss: 0.3851\n",
            "Epoch [49/50], Step [700/782], Loss: 0.3696\n",
            "Epoch [49/50], Training Loss: 0.3844\n",
            "Epoch [49/50], Validation Loss: 1.0093\n",
            "Epoch [50/50], Step [100/782], Loss: 0.5945\n",
            "Epoch [50/50], Step [200/782], Loss: 0.4954\n",
            "Epoch [50/50], Step [300/782], Loss: 0.4689\n",
            "Epoch [50/50], Step [400/782], Loss: 0.4145\n",
            "Epoch [50/50], Step [500/782], Loss: 0.4128\n",
            "Epoch [50/50], Step [600/782], Loss: 0.4737\n",
            "Epoch [50/50], Step [700/782], Loss: 0.2389\n",
            "Epoch [50/50], Training Loss: 0.3839\n",
            "Epoch [50/50], Validation Loss: 1.0143\n",
            "Baseline Model - Time: 2678.67s, Accuracy: 71.41%\n",
            "Epoch [1/50], Step [100/782], Loss: 1.9784\n",
            "Epoch [1/50], Step [200/782], Loss: 1.8858\n",
            "Epoch [1/50], Step [300/782], Loss: 1.6080\n",
            "Epoch [1/50], Step [400/782], Loss: 1.3367\n",
            "Epoch [1/50], Step [500/782], Loss: 1.6248\n",
            "Epoch [1/50], Step [600/782], Loss: 1.2736\n",
            "Epoch [1/50], Step [700/782], Loss: 1.5204\n",
            "Epoch [1/50], Training Loss: 1.6946\n",
            "Epoch [1/50], Validation Loss: 1.2023\n",
            "Epoch [2/50], Step [100/782], Loss: 1.4631\n",
            "Epoch [2/50], Step [200/782], Loss: 1.3715\n",
            "Epoch [2/50], Step [300/782], Loss: 1.3738\n",
            "Epoch [2/50], Step [400/782], Loss: 1.2813\n",
            "Epoch [2/50], Step [500/782], Loss: 1.2457\n",
            "Epoch [2/50], Step [600/782], Loss: 1.2958\n",
            "Epoch [2/50], Step [700/782], Loss: 1.1983\n",
            "Epoch [2/50], Training Loss: 1.3344\n",
            "Epoch [2/50], Validation Loss: 1.1158\n",
            "Epoch [3/50], Step [100/782], Loss: 1.0781\n",
            "Epoch [3/50], Step [200/782], Loss: 1.2890\n",
            "Epoch [3/50], Step [300/782], Loss: 1.1867\n",
            "Epoch [3/50], Step [400/782], Loss: 1.2271\n",
            "Epoch [3/50], Step [500/782], Loss: 1.1680\n",
            "Epoch [3/50], Step [600/782], Loss: 1.2371\n",
            "Epoch [3/50], Step [700/782], Loss: 1.1607\n",
            "Epoch [3/50], Training Loss: 1.1966\n",
            "Epoch [3/50], Validation Loss: 0.9633\n",
            "Epoch [4/50], Step [100/782], Loss: 1.2876\n",
            "Epoch [4/50], Step [200/782], Loss: 1.0893\n",
            "Epoch [4/50], Step [300/782], Loss: 1.2088\n",
            "Epoch [4/50], Step [400/782], Loss: 1.4121\n",
            "Epoch [4/50], Step [500/782], Loss: 1.1643\n",
            "Epoch [4/50], Step [600/782], Loss: 1.0536\n",
            "Epoch [4/50], Step [700/782], Loss: 1.2530\n",
            "Epoch [4/50], Training Loss: 1.1194\n",
            "Epoch [4/50], Validation Loss: 0.9189\n",
            "Epoch [5/50], Step [100/782], Loss: 0.9207\n",
            "Epoch [5/50], Step [200/782], Loss: 0.9522\n",
            "Epoch [5/50], Step [300/782], Loss: 1.1488\n",
            "Epoch [5/50], Step [400/782], Loss: 1.0852\n",
            "Epoch [5/50], Step [500/782], Loss: 0.9848\n",
            "Epoch [5/50], Step [600/782], Loss: 1.1896\n",
            "Epoch [5/50], Step [700/782], Loss: 1.0113\n",
            "Epoch [5/50], Training Loss: 1.0624\n",
            "Epoch [5/50], Validation Loss: 0.9026\n",
            "Epoch [6/50], Step [100/782], Loss: 1.0105\n",
            "Epoch [6/50], Step [200/782], Loss: 1.1755\n",
            "Epoch [6/50], Step [300/782], Loss: 1.2431\n",
            "Epoch [6/50], Step [400/782], Loss: 0.9863\n",
            "Epoch [6/50], Step [500/782], Loss: 1.1017\n",
            "Epoch [6/50], Step [600/782], Loss: 0.8131\n",
            "Epoch [6/50], Step [700/782], Loss: 1.1739\n",
            "Epoch [6/50], Training Loss: 1.0133\n",
            "Epoch [6/50], Validation Loss: 0.8300\n",
            "Epoch [7/50], Step [100/782], Loss: 0.9912\n",
            "Epoch [7/50], Step [200/782], Loss: 0.9685\n",
            "Epoch [7/50], Step [300/782], Loss: 0.9425\n",
            "Epoch [7/50], Step [400/782], Loss: 1.1187\n",
            "Epoch [7/50], Step [500/782], Loss: 0.9994\n",
            "Epoch [7/50], Step [600/782], Loss: 1.0540\n",
            "Epoch [7/50], Step [700/782], Loss: 0.7693\n",
            "Epoch [7/50], Training Loss: 0.9808\n",
            "Epoch [7/50], Validation Loss: 0.7965\n",
            "Epoch [8/50], Step [100/782], Loss: 0.9142\n",
            "Epoch [8/50], Step [200/782], Loss: 1.0383\n",
            "Epoch [8/50], Step [300/782], Loss: 0.7892\n",
            "Epoch [8/50], Step [400/782], Loss: 0.9262\n",
            "Epoch [8/50], Step [500/782], Loss: 1.0272\n",
            "Epoch [8/50], Step [600/782], Loss: 0.9088\n",
            "Epoch [8/50], Step [700/782], Loss: 1.2047\n",
            "Epoch [8/50], Training Loss: 0.9485\n",
            "Epoch [8/50], Validation Loss: 0.7777\n",
            "Epoch [9/50], Step [100/782], Loss: 0.8174\n",
            "Epoch [9/50], Step [200/782], Loss: 0.9778\n",
            "Epoch [9/50], Step [300/782], Loss: 0.9492\n",
            "Epoch [9/50], Step [400/782], Loss: 0.9600\n",
            "Epoch [9/50], Step [500/782], Loss: 0.9569\n",
            "Epoch [9/50], Step [600/782], Loss: 0.7801\n",
            "Epoch [9/50], Step [700/782], Loss: 1.0652\n",
            "Epoch [9/50], Training Loss: 0.9195\n",
            "Epoch [9/50], Validation Loss: 0.7627\n",
            "Epoch [10/50], Step [100/782], Loss: 0.7222\n",
            "Epoch [10/50], Step [200/782], Loss: 0.5496\n",
            "Epoch [10/50], Step [300/782], Loss: 0.7998\n",
            "Epoch [10/50], Step [400/782], Loss: 0.7834\n",
            "Epoch [10/50], Step [500/782], Loss: 0.9679\n",
            "Epoch [10/50], Step [600/782], Loss: 0.7206\n",
            "Epoch [10/50], Step [700/782], Loss: 0.8741\n",
            "Epoch [10/50], Training Loss: 0.8947\n",
            "Epoch [10/50], Validation Loss: 0.7323\n",
            "Epoch [11/50], Step [100/782], Loss: 1.0131\n",
            "Epoch [11/50], Step [200/782], Loss: 0.7082\n",
            "Epoch [11/50], Step [300/782], Loss: 1.2200\n",
            "Epoch [11/50], Step [400/782], Loss: 0.6707\n",
            "Epoch [11/50], Step [500/782], Loss: 0.7232\n",
            "Epoch [11/50], Step [600/782], Loss: 0.7534\n",
            "Epoch [11/50], Step [700/782], Loss: 0.8595\n",
            "Epoch [11/50], Training Loss: 0.8725\n",
            "Epoch [11/50], Validation Loss: 0.7069\n",
            "Epoch [12/50], Step [100/782], Loss: 0.7946\n",
            "Epoch [12/50], Step [200/782], Loss: 0.8434\n",
            "Epoch [12/50], Step [300/782], Loss: 0.8925\n",
            "Epoch [12/50], Step [400/782], Loss: 0.8294\n",
            "Epoch [12/50], Step [500/782], Loss: 0.8262\n",
            "Epoch [12/50], Step [600/782], Loss: 0.4648\n",
            "Epoch [12/50], Step [700/782], Loss: 0.7162\n",
            "Epoch [12/50], Training Loss: 0.8573\n",
            "Epoch [12/50], Validation Loss: 0.7401\n",
            "Epoch [13/50], Step [100/782], Loss: 0.9819\n",
            "Epoch [13/50], Step [200/782], Loss: 0.8331\n",
            "Epoch [13/50], Step [300/782], Loss: 1.1226\n",
            "Epoch [13/50], Step [400/782], Loss: 0.6576\n",
            "Epoch [13/50], Step [500/782], Loss: 0.7154\n",
            "Epoch [13/50], Step [600/782], Loss: 0.7795\n",
            "Epoch [13/50], Step [700/782], Loss: 0.9618\n",
            "Epoch [13/50], Training Loss: 0.8348\n",
            "Epoch [13/50], Validation Loss: 0.7172\n",
            "Epoch [14/50], Step [100/782], Loss: 0.8635\n",
            "Epoch [14/50], Step [200/782], Loss: 0.7071\n",
            "Epoch [14/50], Step [300/782], Loss: 0.7504\n",
            "Epoch [14/50], Step [400/782], Loss: 0.9328\n",
            "Epoch [14/50], Step [500/782], Loss: 0.6587\n",
            "Epoch [14/50], Step [600/782], Loss: 0.8607\n",
            "Epoch [14/50], Step [700/782], Loss: 0.9899\n",
            "Epoch [14/50], Training Loss: 0.8243\n",
            "Epoch [14/50], Validation Loss: 0.7320\n",
            "Epoch [15/50], Step [100/782], Loss: 0.7773\n",
            "Epoch [15/50], Step [200/782], Loss: 0.8252\n",
            "Epoch [15/50], Step [300/782], Loss: 1.1010\n",
            "Epoch [15/50], Step [400/782], Loss: 0.4700\n",
            "Epoch [15/50], Step [500/782], Loss: 0.7577\n",
            "Epoch [15/50], Step [600/782], Loss: 0.6976\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xuFiq62zN5OQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3ZuaSI73N5R8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fufVxjVcN5VX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "\n",
        "# Sample CSV data\n",
        "csv_data = \"\"\"\n",
        "data,label\n",
        "1.0,0\n",
        "2.0,1\n",
        "3.0,0\n",
        "4.0,1\n",
        "\"\"\"\n",
        "\n",
        "# Write the sample CSV data to a file\n",
        "with open('data.csv', 'w') as f:\n",
        "    f.write(csv_data)\n",
        "\n",
        "# Custom Dataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, csv_file):\n",
        "        self.data_frame = pd.read_csv(csv_file)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_frame)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data = self.data_frame.iloc[idx, 0]  # Get the data value at the given index\n",
        "        label = self.data_frame.iloc[idx, 1]  # Get the label value at the given index\n",
        "        sample = {'data': torch.tensor(data, dtype=torch.float32), 'label': torch.tensor(label, dtype=torch.long)}\n",
        "        return sample\n",
        "\n",
        "# Create Dataset and DataLoader\n",
        "dataset = CustomDataset(csv_file='data.csv')\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "# Iterate through the DataLoader\n",
        "for i, batch in enumerate(dataloader):\n",
        "    print(f\"Batch {i}:\")\n",
        "    print(batch)\n"
      ],
      "metadata": {
        "id": "5qLFbj_zo6U6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YAThSAA6N6jX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Transfer Learning in PyTorch**\n",
        "Transfer learning leverages pre-trained models to solve new tasks, especially useful for tasks with limited labeled data. This method reduces training time and improves performance.\n",
        "\n",
        "###**Key Concepts**\n",
        "**Pre-trained Models:** Models trained on large datasets.\n",
        "**Feature Extraction:** Using pre-trained models to extract features.\n",
        "**Fine-Tuning:** Slightly adjusting pre-trained model weights.\n",
        "Steps for Implementing Transfer Learning\n",
        "\n",
        "**1. Data Transforms**\n",
        "\n",
        "We define transformations for the training and validation data. These transformations include resizing, cropping, normalizing, and data augmentation techniques.\n"
      ],
      "metadata": {
        "id": "SoS2i1rpN6nA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "clgxp3UCN67V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Goal and Key Concepts of the Program\n",
        "\n",
        "The main goal of the following program is to demonstrate **transfer learning** using a pre-trained Convolutional Neural Network (CNN) model (ResNet18) for a custom image classification task. Transfer learning leverages a model pre-trained on a large dataset (like ImageNet) and fine-tunes it to work on a smaller, specific dataset.\n",
        "\n",
        "## Key Concepts Explained in the Program:\n",
        "\n",
        "### 1. Transfer Learning\n",
        "- Reuse of a pre-trained model on a new task.\n",
        "- Fine-tuning the model to adapt it to the new task.\n",
        "\n",
        "### 2. Data Preparation\n",
        "- Applying appropriate transformations (resizing, cropping, normalization) to the training and validation datasets to ensure the data is in the correct format for the pre-trained model.\n",
        "\n",
        "### 3. Model Modification\n",
        "- Loading a pre-trained ResNet18 model.\n",
        "- Modifying the final fully connected (FC) layer to match the number of classes in the new dataset.\n",
        "\n",
        "### 4. Training and Evaluation\n",
        "- Setting up the training loop with a training and validation phase.\n",
        "- Using an optimizer (SGD) and a learning rate scheduler (StepLR) to optimize the model.\n",
        "- Tracking and saving the best-performing model based on validation accuracy.\n",
        "\n",
        "### 5. Utilizing PyTorch\n",
        "- Using PyTorch's built-in functionalities for data loading (`DataLoader`), transformations (`transforms`), and model training (`nn`, `optim`).\n",
        "\n",
        "## Detailed Explanation:\n",
        "\n",
        "### 1. Transfer Learning\n",
        "- **Concept**: Instead of training a CNN from scratch, which is computationally expensive and requires a large amount of labeled data, we use a model pre-trained on a large dataset (ImageNet). The pre-trained model has already learned useful features, such as edges and textures, which can be applied to a new but related task.\n",
        "- **Implementation**: We load a pre-trained ResNet18 model and replace its final FC layer with a new one that matches the number of classes in our specific dataset.\n",
        "\n",
        "### 2. Data Preparation\n",
        "- **Transformations**: Data augmentation techniques like random cropping and horizontal flipping are applied to the training data to make the model robust to variations. Both training and validation data are normalized to match the distribution of the ImageNet dataset, ensuring the pre-trained model's weights are applied effectively.\n",
        "\n",
        "### 3. Model Modification\n",
        "- **Loading Pre-trained Model**: The ResNet18 model pre-trained on ImageNet is loaded.\n",
        "- **Replacing FC Layer**: The original FC layer, which outputs 1000 classes (ImageNet), is replaced with a new FC layer that outputs the number of classes in the custom dataset.\n",
        "\n",
        "### 4. Training and Evaluation\n",
        "- **Training Loop**: The model is trained over multiple epochs. In each epoch, it goes through both training and validation phases.\n",
        "- **Optimization**: An SGD optimizer with momentum and a StepLR scheduler are used to optimize the model parameters.\n",
        "- **Tracking Performance**: During training, the model's performance is tracked by calculating loss and accuracy for both training and validation sets. The model's state is saved if it achieves the best validation accuracy.\n",
        "\n",
        "### 5. Using PyTorch\n",
        "- **DataLoader**: Efficiently loads data in batches, shuffling the training data to ensure randomness.\n",
        "- **Transforms**: Provides a way to apply transformations to images, ensuring they are in the correct format and normalized.\n",
        "- **Neural Network Module (`nn`)**: Defines the network architecture and the loss function.\n",
        "- **Optimizer Module (`optim`)**: Provides optimization algorithms to adjust the model weights during training.\n",
        "\n",
        "## Concepts covered\n",
        "\n",
        "The main concept this program aims to explain is **transfer learning** with a focus on practical implementation using PyTorch. It shows how to:\n",
        "- Prepare data for transfer learning.\n",
        "- Modify a pre-trained model to suit a new classification task.\n",
        "- Train and evaluate the modified model efficiently using PyTorch.\n",
        "\n",
        "By understanding and implementing this program, one can leverage powerful pre-trained models for their specific tasks, saving time and resources while achieving high performance.\n"
      ],
      "metadata": {
        "id": "i8D8-3CmN7Q2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "import copy\n",
        "import os\n",
        "\n",
        "# Data Transforms\n",
        "# Define transformations for the training and validation datasets.\n",
        "# These include resizing, cropping, normalizing, and data augmentation for training data.\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),  # Randomly crop the image to 224x224 pixels\n",
        "        transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally\n",
        "        transforms.ToTensor(),  # Convert the image to a tensor\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],  # Normalize the image with mean and std deviation\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),  # Resize the image to 256 pixels\n",
        "        transforms.CenterCrop(224),  # Center crop the image to 224x224 pixels\n",
        "        transforms.ToTensor(),  # Convert the image to a tensor\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],  # Normalize the image with mean and std deviation\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "# Load Data\n",
        "# Load the training and validation datasets using ImageFolder and apply the transformations defined above.\n",
        "# Create DataLoader objects to facilitate batching and shuffling of the data.\n",
        "data_dir = 'data/hymenoptera_data'\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n",
        "                  for x in ['train', 'val']}\n",
        "dataloaders = {x: DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=4)\n",
        "               for x in ['train', 'val']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "class_names = image_datasets['train'].classes\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available, otherwise use CPU\n",
        "\n",
        "# Load Pre-trained Model\n",
        "# Load the ResNet18 model pre-trained on ImageNet.\n",
        "# Modify the final fully connected layer to match the number of classes in the new dataset.\n",
        "model_ft = models.resnet18(pretrained=True)\n",
        "num_ftrs = model_ft.fc.in_features  # Get the number of input features to the fully connected layer\n",
        "model_ft.fc = nn.Linear(num_ftrs, len(class_names))  # Replace the FC layer with a new one with the correct number of output features\n",
        "\n",
        "model_ft = model_ft.to(device)  # Move the model to the GPU or CPU\n",
        "criterion = nn.CrossEntropyLoss()  # Define the loss function\n",
        "\n",
        "# Set Up Optimizer and Scheduler\n",
        "# Set up the optimizer (SGD) and the learning rate scheduler (StepLR).\n",
        "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
        "\n",
        "# Training and Evaluation Functions\n",
        "# Define the function to train and evaluate the model.\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()  # Track the time taken to train the model\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())  # Initialize best model weights\n",
        "    best_acc = 0.0  # Initialize best accuracy\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()  # Set model to evaluation mode\n",
        "\n",
        "            running_loss = 0.0  # Initialize running loss\n",
        "            running_corrects = 0  # Initialize running corrects\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()  # Zero the parameter gradients\n",
        "\n",
        "                # Forward pass\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # Backward pass + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # Statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            if phase == 'train':\n",
        "                scheduler.step()  # Step the learning rate scheduler\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]  # Calculate epoch loss\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]  # Calculate epoch accuracy\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "            # Deep copy the model if it has the best accuracy\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Best val Acc: {best_acc:4f}')\n",
        "\n",
        "    # Load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n",
        "# Train the Model\n",
        "# Call the train_model function to train the model with the specified parameters.\n",
        "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=25)\n",
        "\n",
        "# Save the Model\n",
        "# Save the trained model's state dictionary to a file.\n",
        "torch.save(model_ft.state_dict(), 'model_ft.pth')\n"
      ],
      "metadata": {
        "id": "JpNt-X1gTMd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1-U5wLnmXW7l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Using Different Pre-trained Models**\n",
        "**VGG16:**"
      ],
      "metadata": {
        "id": "EbecsYSJXXLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "# Load pre-trained VGG16 model\n",
        "model_vgg16 = models.vgg16(pretrained=True)\n",
        "\n",
        "# Freeze all layers\n",
        "for param in model_vgg16.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Replace the final fully connected layer\n",
        "num_ftrs = model_vgg16.classifier[6].in_features\n",
        "model_vgg16.classifier[6] = nn.Linear(num_ftrs, len(class_names))\n",
        "\n",
        "# Move the model to the device\n",
        "model_vgg16 = model_vgg16.to(device)\n"
      ],
      "metadata": {
        "id": "tyIFCwlLXgEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inception v3:**"
      ],
      "metadata": {
        "id": "EBLhowm9XjP4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "# Load pre-trained Inception v3 model\n",
        "model_inception = models.inception_v3(pretrained=True)\n",
        "\n",
        "# Freeze all layers\n",
        "for param in model_inception.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Replace the final fully connected layer\n",
        "num_ftrs = model_inception.fc.in_features\n",
        "model_inception.fc = nn.Linear(num_ftrs, len(class_names))\n",
        "\n",
        "# Move the model to the device\n",
        "model_inception = model_inception.to(device)\n"
      ],
      "metadata": {
        "id": "ALRptyoHXm-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DenseNet:**"
      ],
      "metadata": {
        "id": "OnK5VZOAXqeg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "# Load pre-trained DenseNet model\n",
        "model_densenet = models.densenet121(pretrained=True)\n",
        "\n",
        "# Freeze all layers\n",
        "for param in model_densenet.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Replace the final fully connected layer\n",
        "num_ftrs = model_densenet.classifier.in_features\n",
        "model_densenet.classifier = nn.Linear(num_ftrs, len(class_names))\n",
        "\n",
        "# Move the model to the device\n",
        "model_densenet = model_densenet.to(device)\n"
      ],
      "metadata": {
        "id": "GrpxjzjBXv79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MobileNet v2:"
      ],
      "metadata": {
        "id": "6Cola60_X1jh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "# Load pre-trained MobileNet v2 model\n",
        "model_mobilenet = models.mobilenet_v2(pretrained=True)\n",
        "\n",
        "# Freeze all layers\n",
        "for param in model_mobilenet.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Replace the final fully connected layer\n",
        "num_ftrs = model_mobilenet.classifier[1].in_features\n",
        "model_mobilenet.classifier[1] = nn.Linear(num_ftrs, len(class_names))\n",
        "\n",
        "# Move the model to the device\n",
        "model_mobilenet = model_mobilenet.to(device)\n"
      ],
      "metadata": {
        "id": "161MgRdrYEaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AstKRe74YGoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Multiple pretrained models"
      ],
      "metadata": {
        "id": "PSXFLaHGbD73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "# List of models to use\n",
        "models_list = ['vgg16', 'inception_v3', 'densenet121', 'mobilenet_v2']\n",
        "\n",
        "for model_name in models_list:\n",
        "    # Load the pre-trained model\n",
        "    model = getattr(models, model_name)(pretrained=True)\n",
        "\n",
        "    # Freeze all the layers\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Replace the final fully connected layer\n",
        "    if model_name == 'vgg16':\n",
        "        num_ftrs = model.classifier[6].in_features\n",
        "        model.classifier[6] = nn.Linear(num_ftrs, len(class_names))\n",
        "    elif model_name == 'inception_v3':\n",
        "        num_ftrs = model.fc.in_features\n",
        "        model.fc = nn.Linear(num_ftrs, len(class_names))\n",
        "    elif model_name == 'densenet121':\n",
        "        num_ftrs = model.classifier.in_features\n",
        "        model.classifier = nn.Linear(num_ftrs, len(class_names))\n",
        "    elif model_name == 'mobilenet_v2':\n",
        "        num_ftrs = model.classifier[1].in_features\n",
        "        model.classifier[1] = nn.Linear(num_ftrs, len(class_names))\n",
        "\n",
        "    # Move the model to the device\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Define the loss function\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Set up the optimizer and scheduler\n",
        "    optimizer = optim.SGD(model.classifier.parameters(), lr=0.001, momentum=0.9)\n",
        "    exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "    # Train the model\n",
        "    model = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=25)\n",
        "\n",
        "    # Save the model\n",
        "    torch.save(model.state_dict(), f'model_{model_name}.pth')\n"
      ],
      "metadata": {
        "id": "iWs2HJ39bJb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning with Pre-trained ResNet18 on CIFAR-10 Dataset"
      ],
      "metadata": {
        "id": "ZRgu7k2cbK05"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Setup and Imports**"
      ],
      "metadata": {
        "id": "ncBzJHnvdNS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "import copy\n",
        "import os\n",
        "\n",
        "# Check device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "8aa8Zjv1czSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Data Preparation**\n",
        "\n",
        "We'll use CIFAR-10 and pretend it is our custom dataset.\n",
        "\n",
        "We'll split it into training and validation sets."
      ],
      "metadata": {
        "id": "LaluHEHidY_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data augmentation and normalization for training\n",
        "# Just normalization for validation\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "data_dir = './data'\n",
        "trainset = datasets.CIFAR10(root=data_dir, train=True, download=True, transform=data_transforms['train'])\n",
        "trainloader = DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)\n",
        "\n",
        "valset = datasets.CIFAR10(root=data_dir, train=False, download=True, transform=data_transforms['val'])\n",
        "valloader = DataLoader(valset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "dataloaders = {'train': trainloader, 'val': valloader}\n",
        "dataset_sizes = {'train': len(trainset), 'val': len(valset)}\n",
        "class_names = trainset.classes\n"
      ],
      "metadata": {
        "id": "ZzN6p5TNddRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Model Preparation**\n",
        "\n",
        "Load a pre-trained ResNet18 model and modify the final layer to fit our custom dataset"
      ],
      "metadata": {
        "id": "YJBPTdTAecvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained ResNet18 model\n",
        "model_ft = models.resnet18(pretrained=True)\n",
        "\n",
        "# Freeze all the layers in the pre-trained model\n",
        "for param in model_ft.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Modify the fully connected layer to match the number of classes in the new dataset\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "model_ft.fc = nn.Linear(num_ftrs, len(class_names))\n",
        "\n",
        "# Move the model to the specified device (GPU or CPU)\n",
        "model_ft = model_ft.to(device)\n"
      ],
      "metadata": {
        "id": "9qkRQv2wdUkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Training**\n",
        "\n",
        "Define the loss function, optimizer, and training loop."
      ],
      "metadata": {
        "id": "g2rFlaBJerkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_ft = optim.SGD(model_ft.fc.parameters(), lr=0.001, momentum=0.9)\n",
        "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
        "\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=10):\n",
        "    since = time.time()\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Best val Acc: {best_acc:4f}')\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n",
        "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=10)\n"
      ],
      "metadata": {
        "id": "YsHO1UnyeiYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5: Evaluation**\n",
        "\n",
        "Evaluate the model's performance on the test set."
      ],
      "metadata": {
        "id": "dNep4NRkewrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, dataloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    print(f'Accuracy of the network on the {total} test images: {100 * correct / total:.2f}%')\n",
        "\n",
        "test_model(model_ft, valloader)\n"
      ],
      "metadata": {
        "id": "S3rOhCeYe7N6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6: Visualization**\n",
        "\n",
        "Visualize some predictions made by the model."
      ],
      "metadata": {
        "id": "52ROFJwWfDa5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# Get some random test images\n",
        "dataiter = iter(valloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# Print images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "print('GroundTruth: ', ' '.join(f'{class_names[labels[j]]}' for j in range(4)))\n",
        "\n",
        "# Predictions\n",
        "images = images.to(device)\n",
        "outputs = model_ft(images)\n",
        "_, predicted = torch.max(outputs, 1)\n",
        "\n",
        "print('Predicted: ', ' '.join(f'{class_names[predicted[j]]}' for j in range(4)))\n"
      ],
      "metadata": {
        "id": "IMUDJ57CfF1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bA_BPEfIe9t9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Transfer Learning with Multiple Pre-trained Models on CIFAR-10 Dataset**"
      ],
      "metadata": {
        "id": "ss_jmyY0fmZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "import copy\n",
        "\n",
        "# Check device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Data Preparation\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "data_dir = './data'\n",
        "trainset = torchvision.datasets.CIFAR10(root=data_dir, train=True, download=True, transform=data_transforms['train'])\n",
        "trainloader = DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)\n",
        "\n",
        "valset = torchvision.datasets.CIFAR10(root=data_dir, train=False, download=True, transform=data_transforms['val'])\n",
        "valloader = DataLoader(valset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "dataloaders = {'train': trainloader, 'val': valloader}\n",
        "dataset_sizes = {'train': len(trainset), 'val': len(valset)}\n",
        "class_names = trainset.classes\n",
        "\n",
        "# Function to prepare a model\n",
        "def prepare_model(model_name, num_classes, feature_extract=True):\n",
        "    if model_name == \"resnet\":\n",
        "        model = models.resnet18(pretrained=True)\n",
        "        set_parameter_requires_grad(model, feature_extract)\n",
        "        num_ftrs = model.fc.in_features\n",
        "        model.fc = nn.Linear(num_ftrs, num_classes)\n",
        "    elif model_name == \"vgg\":\n",
        "        model = models.vgg16(pretrained=True)\n",
        "        set_parameter_requires_grad(model, feature_extract)\n",
        "        num_ftrs = model.classifier[6].in_features\n",
        "        model.classifier[6] = nn.Linear(num_ftrs, num_classes)\n",
        "    elif model_name == \"densenet\":\n",
        "        model = models.densenet121(pretrained=True)\n",
        "        set_parameter_requires_grad(model, feature_extract)\n",
        "        num_ftrs = model.classifier.in_features\n",
        "        model.classifier = nn.Linear(num_ftrs, num_classes)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid model name\")\n",
        "\n",
        "    return model.to(device)\n",
        "\n",
        "# Function to freeze layers\n",
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "# Prepare models\n",
        "models_list = {\n",
        "    'resnet': prepare_model('resnet', len(class_names)),\n",
        "    'vgg': prepare_model('vgg', len(class_names)),\n",
        "    'densenet': prepare_model('densenet', len(class_names))\n",
        "}\n",
        "\n",
        "# Training Function\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=10):\n",
        "    since = time.time()\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Best val Acc: {best_acc:4f}')\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n",
        "# Train each model\n",
        "for model_name, model in models_list.items():\n",
        "    print(f\"Training {model_name} model...\")\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "    exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "    models_list[model_name] = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=10)\n",
        "    print(f\"Finished training {model_name} model.\\n\")\n",
        "\n",
        "# Evaluation Function\n",
        "def test_model(model, dataloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    print(f'Accuracy of the network on the {total} test images: {100 * correct / total:.2f}%')\n",
        "\n",
        "# Evaluate each model\n",
        "for model_name, model in models_list.items():\n",
        "    print(f\"Evaluating {model_name} model...\")\n",
        "    test_model(model, valloader)\n",
        "    print(f\"Finished evaluating {model_name} model.\\n\")\n",
        "\n",
        "# Visualization\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# Visualize predictions\n",
        "def visualize_model(model, num_images=6):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(valloader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "                ax.axis('off')\n",
        "                ax.set_title(f'predicted: {class_names[preds[j]]}')\n",
        "                imshow(inputs.cpu().data[j])\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "        model.train(mode=was_training)\n",
        "\n",
        "# Visualize predictions for each model\n",
        "for model_name, model in models_list.items():\n",
        "    print(f\"Visualizing predictions for {model_name} model...\")\n",
        "    visualize_model(model)\n",
        "    print(f\"Finished visualizing {model_name} model.\\n\")\n"
      ],
      "metadata": {
        "id": "qBy7N9CTfyRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oG5VAFk5jPIc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving and Loading Models in PyTorch\n",
        "\n",
        "### Introduction\n",
        "PyTorch provides multiple ways to save and load models. Choosing the right method depends on your specific needs.\n",
        "\n",
        "### Saving Methods\n",
        "\n",
        "#### 1. Saving the Entire Model\n",
        "Saves the whole model, including its architecture and parameters.\n",
        "```python\n",
        "torch.save(model, 'model.pth')\n"
      ],
      "metadata": {
        "id": "oKXDo-DPjPX_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SVe-6DT5jQTH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}